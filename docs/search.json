[
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "resume",
    "section": "",
    "text": "This webpage is undergoing construction. Resume will be updated shortly."
  },
  {
    "objectID": "posts/2022-10-24-first-blog-test/index.html",
    "href": "posts/2022-10-24-first-blog-test/index.html",
    "title": "First blog post",
    "section": "",
    "text": "CitationBibTeX citation:@online{mccamy2022,\n  author = {Colleen McCamy},\n  title = {First Blog Post},\n  date = {2022-11-12},\n  url = {https://colleenmccamy.github.io/2022-11-12-first-blog-post},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nColleen McCamy. 2022. “First Blog Post.” November 12, 2022.\nhttps://colleenmccamy.github.io/2022-11-12-first-blog-post."
  },
  {
    "objectID": "posts/2022-11-15-lights-out-in-texas/index.html",
    "href": "posts/2022-11-15-lights-out-in-texas/index.html",
    "title": "Lights Out in Texas",
    "section": "",
    "text": "time to code\nThis section illustrates the code executed to explore our questions.\nHeading to the library\nGet your library card ready because it is time to load these packages.\n\n\ncheckout the code\nlibrary(dplyr)\nlibrary(sf)\nlibrary(stars)\nlibrary(tmap)\nlibrary(raster)\nlibrary(terra)\nlibrary(tmap)\nlibrary(ggplot2)\n\n\nStep 1: Diving into the Data\nWhat would be a data science project without the data? The following code block uses SQL and the stars package to load in the data needed for the project.\nFor our raster data we used NASA’s Worldview data for February 7, 2021 (before the power outage) and February 16, 2021 (during the power outage) to visualize the extent of the power outage in the Houston area. These days were selected as other days during the time frame had too much cloud cover to be useful.\nVIIRS data is distributed through NASA’s Level-1 and Atmospheric Archive & Distribution System Distributed Active Archive Center (LAADS DAAC). Data was downloaded and prepped in advance to this assignment.\n\n\ncheckout the code\n#reading in the NASA raster files\nnl_feb07_t1<- read_stars('/Users/colleenmccamy/Documents/MEDS/EDS_223_Spatial_Data/data/assignment3/VNP46A1/VNP46A1.A2021038.h08v05.001.2021039064328.tif')\n\nnl_feb07_t2 <- read_stars('/Users/colleenmccamy/Documents/MEDS/EDS_223_Spatial_Data/data/assignment3/VNP46A1/VNP46A1.A2021038.h08v06.001.2021039064329.tif')\n\nnl_feb16_t1 <-read_stars('/Users/colleenmccamy/Documents/MEDS/EDS_223_Spatial_Data/data/assignment3/VNP46A1/VNP46A1.A2021047.h08v05.001.2021048091106.tif')\n\nnl_feb16_t2 <- read_stars('/Users/colleenmccamy/Documents/MEDS/EDS_223_Spatial_Data/data/assignment3/VNP46A1/VNP46A1.A2021047.h08v06.001.2021048091105.tif')\n\n\nNext up we have data from roads. To minimize accounting for light from major highways systems, we used publicly available geographic data from OpenStreetMap (OSM) through Geofabrik’s download sites. This data were downloaded and prepped in advance to contain a subset of highway and raods that intersect with the Houston metropolitan area. We also used data from Geofabrik’s download sites for information on houses in the Houston metropolitan area.\n\n\ncheckout the code\n## ---- Highway Data\n# reading in highway data using SQL query\nquery <- \"SELECT * FROM gis_osm_roads_free_1 WHERE fclass='motorway'\"\n\n# reading in the highways data with st_read()\nhighways <- st_read(\"/Users/colleenmccamy/Documents/MEDS/EDS_223_Spatial_Data/data/assignment3/gis_osm_roads_free_1.gpkg\", query = query)\n\n\n## ---- Houses Data\n# defining the query for the houses\nquery_houses <- \"SELECT * FROM gis_osm_buildings_a_free_1 WHERE (type IS NULL AND name IS NULL) OR type in ('residential', 'apartments', 'house', 'static_caravan', 'detached')\"\n\n# reading in the highways data with st_read()\nhouses <- st_read(\"/Users/colleenmccamy/Documents/MEDS/EDS_223_Spatial_Data/data/assignment3/gis_osm_buildings_a_free_1.gpkg\", query = query_houses)\n\n\nLastly, we use data from the U.S. Census Bureau’s American Community Survey for census tracts in 2019 from an ArcGIS file geodatabase. The metadata for each layer is available at ACS metadata and this data was downloaded in advance to this investigation.\n\n\ncheckout the code\n#reading in geometry data and selecting for the layer containing the geometry\ncensus_geom <- st_read(\"/Users/colleenmccamy/Documents/MEDS/EDS_223_Spatial_Data/data/assignment3/ACS_2019_5YR_TRACT_48_TEXAS.gdb\", layer = \"ACS_2019_5YR_TRACT_48_TEXAS\")\n\n#reading in income data\nincome_median <- st_read(\"/Users/colleenmccamy/Documents/MEDS/EDS_223_Spatial_Data/data/assignment3/ACS_2019_5YR_TRACT_48_TEXAS.gdb\", layer = \"X19_INCOME\")\n\n\nStep 2: Creating a blackout mask\nThe following code outlines how I created a blackout mask for the Houston area that I could use for identifying impacted homes. We operated under the assumption that any location that experienced a drop of more than 200 nW cm-2sr-1 experienced a blackout.\n\n\ncheckout the code\n# combing the data into single stars objects for each day\nfeb16_tile <- st_mosaic(nl_feb16_t1, nl_feb16_t2)\nfeb07_tile <- st_mosaic(nl_feb07_t1, nl_feb07_t2)\n\n# adding an indicator of the attributes in the data\nfeb_16_tile_names = setNames(feb16_tile, \"light_16\")\nfeb_07_tile_names = setNames(feb07_tile, \"light_07\")\n\n# matrix alegbra to calculate the difference light difference between the two dates \nblackout_dif <- feb_07_tile_names - feb_16_tile_names\n\n# #filtering for the differences of a drop less that 200 nW cm-2sr-1 as NA\nblackout_mask <- cut(blackout_dif, c(200, Inf), labels = \"outage\")\n\n# vectorizing the blackout mask and fixing any invalid geometries\nblackout_mask_v <- st_as_sf(blackout_mask) |> \n  st_make_valid()\n\n# creating a polygon of Houston's coordinates \nhou_border <- st_polygon(list(rbind(c(-96.5,29), c(-96.5,30.5), c(-94.5, 30.5), c(-94.5,29), c(-96.5,29))))\n\n# converting to an sf object and identifying the coordinate reference system\nhou_border_sf <- st_sfc(hou_border, crs = 'EPSG:4326')\n\n# cropping the blackout mask with the Houston polygon\nhou_outage_mask_v <- blackout_mask_v[hou_border_sf, ,]\n\n# reprojectting the cropped object to a new crs and converting it as an sf object\nhou_outage_mask_v_3083 <- st_transform(hou_outage_mask_v, crs = 'EPSG:3083')\noutage_mask_clean <- st_as_sf(hou_outage_mask_v_3083)\n\n\nStep 3: Excluding highway data\nTo exclude light from highways, we created a buffer of 200 meters and kept areas in our blackout mask that were greater than 200 meters away from a highway.\n\n\ncheckout the code\n# selecting the highway geometry data\nhighways_geom <- highways$geom\n\n# transforming the highway geometries to the consistent crs\nhighways_geom <- st_transform(highways_geom, crs = 'EPSG:3083')\n\n# creating a buffer zone for highways geometry data of 200 meters\nhighway_buffer <- st_buffer(x = highways_geom, dist = 200)\nhighway_buffer <- st_transform(highway_buffer, crs = 'EPSG:3083')\n\n# combining the geometries into one and creating a mask that excludes the highway data\nhighway_buffer <- st_union(highway_buffer, by_feature = FALSE)\nmask_hou_highway <- outage_mask_clean[highway_buffer, , op = st_disjoint]\n\n\nStep 4: Identifying Impacted Homes\nTo find the number of impacted homes, the code below outlines how I used the new blackout mask with the highway data to identify homes most likely impacted by the power outage.\n\n\ncheckout the code\n# transforming houses data to be usable\nhouses <- st_transform(houses, crs = 'EPSG:3083')\nhouses_st <- st_as_sf(houses)\n\n# filtering the houses data with the blackout mask\noutage_houses <- houses_st[mask_hou_highway, drop = FALSE]\n\n# identifying how many homes were affected\nprint(paste0(\"There were \", nrow(outage_houses), \" homes affected by the power outage on Feburary 16, 2021.\"))\n\n\nStep 4: Investigating Socioeconomic factors\nNow that we have information on the houses that were affected we can match this with the socioeconomic census tract information and determine which census tracts were impacted by the power outage.\n\n\ncheckout the code\n# transforming the census data to be consistent with the crs\ncensus_geom <- st_transform(census_geom, crs = 'EPSG:3083')\n\n#selecting the necessary variables and renaming for clarity\nincome_med_select <- income_median |> \n  dplyr::select(\"GEOID\", \"B19013e1\") |> \n  rename(GEOID_Data = GEOID, median_income = B19013e1)\n\n# changing the income object to a data_frame\nincome_med_select_df <- tibble(income_med_select)\n\n# joining census geometries and median income data\ncensus_data <- left_join(census_geom, \n                         income_med_select, \n                         by = \"GEOID_Data\")\n\n# transforming both objects to the correct crs\ncensus_data <- st_transform(census_data, crs = 'EPSG:4326')\noutage_houses <- st_transform(outage_houses, crs = 'EPSG:4326')\n\n# filtering the census data using the outage houses and adding column indicating that these census tracts were part of a blackout\ncensus_outage <- sf::st_filter(census_data, outage_houses) |> \n  mutate(blackout = 'yes')\n\n\nStep 5: Comparing the incomes of impacted tracts and unimpacted tracts\nIt is time to visualize our findings. This code breaks down the data wrangling needed for the visualizations and the maps and plots created to compare which census tracts experienced a blackout vs the census tracts that did not experience a blackout.\n\n\ncheckout the code\n## --- Wrangling our data for our visualizations -----------\n\n# transforming both objects to the crs 4326 to crop it\ncensus_data <- st_transform(census_data, crs = 'EPSG:4326')\nhou_border_sf <- st_transform(hou_border_sf, crs = 'EPSG:4326')\n\n# cropping the census data with the Houston border for filtering\ncensus_data_hou <- census_data[hou_border_sf, ,] \n\n# transforming census data back to the EPSG:3083 crs\ncensus_data_hou <- st_transform(census_data_hou, crs = 'EPSG:3083')\n\n# selecting necessary columns for houston census data\ncensus_data_hou <- census_data_hou |> \n  dplyr::select(\"NAMELSAD\", \"Shape\", \"median_income\", \"GEOID_Data\")\n\n# selecting necessary columns for outage data by census track\ncensus_outage <- census_outage |> \n  dplyr::select(\"blackout\", \"GEOID_Data\")\ncensus_outage_map <- census_outage |> \n  dplyr::select(\"blackout\")\n\n# converting census outage data to a dataframe in order to join\ncensus_outage_df <- as.data.frame(census_outage)\n\n# joining census outage data and census data for all of Houston\ncensus_map_data <- left_join(census_data_hou, \n                             census_outage_df, \n                             by = \"GEOID_Data\")\n\ncensus_map_data <- census_map_data |> \n  dplyr::select('median_income', 'blackout')\n\n# converting census map data to a dataframe to plot\ncensus_plot_data <- data_frame(census_map_data)\n\n# adding an indicator for homes that didn't experience a blackout\ncensus_plot_data <- census_plot_data |> \n  mutate(blackout = replace(blackout, is.na(blackout), \"no\"))\n\n# creating a data frame for homes that experienced a blackout to plot\ncensus_plot_data_blackout <- census_plot_data |> \n  dplyr::select(\"median_income\", \"blackout\") |> \n  filter(blackout == \"yes\")\n\n\n# creating a data frame for homes that didn't experienced a blackout to plot\ncensus_plot_data_no_blackout <- census_plot_data |> \n  dplyr::select(\"median_income\", \"blackout\") |> \n  filter(blackout == \"no\")\n\n\n\n\ncheckout the code\n## ------- Mapping our data ---------------\n\n# changing the view mode to be interactive\ntmap_mode(\"view\")\n\n\ntmap mode set to interactive viewing\n\n\ncheckout the code\n# mapping median income by census track and identifying outages by dots\ntm_shape(census_map_data) +\n  tm_polygons(col = \"median_income\",\n              palette = c(\"#227c9d\", \n                          \"#17c3b2\", \n                          \"#ffcb77\", \n                          \"#ffe2b3\", \n                          \"#feb3b1\", \n                          \"#fe6d73\"),\n              textNA = \"Missing Income Data\", \n              colorNA = \"#e4ebea\",\n              title = \"Median Income\") +\n  tm_shape(census_outage_map) +\n  tm_dots(shape = 1,\n          title = 'blackout') +\n  tm_layout(main.title = \"Houston Census Data by Income that Experienced A Power Outage\",\n            legend.outside = TRUE,\n            main.title.size = 1\n            )\n\n\n\n\n\n\n\nThe dots indicate which census tracts were impacted by the blackout.\n\n\ncheckout the code\n### ---- Plotting our data --------\n\n# plotting census data that experienced a blackout\nggplot(census_plot_data_blackout, aes(x = median_income)) +\n  geom_histogram(color = \"#3d5a80\",fill = \"#98c1d9\") +\n  labs(title = \"Median Income for Homes that Experienced a Blackout\",\n       x = \"Median Income\",\n       y = \"Count\") +\n  theme_minimal()\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 3 rows containing non-finite values (stat_bin).\n\n\n\n\n\ncheckout the code\n# plotting census data that didn't experienced a blackout\nggplot(census_plot_data_no_blackout, aes(x = median_income)) +\n  geom_histogram(fill = \"#81b29a\",\n                 color = \"#335c67\") +\n  labs(title = \"Median Income for Homes that Didn't Experience a Blackout\",\n       x = \"Median Income\",\n       y = \"Count\") +\n  theme_minimal()\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 7 rows containing non-finite values (stat_bin).\n\n\n\n\n\ncheckout the code\n# plotting the comparison data via geom jitter plot\nggplot(census_plot_data, aes(x = blackout, y = median_income)) +\n  geom_jitter(width = 0.1,\n              height = 0,\n              color = \"#248577\",\n              alpha = 0.8) +\n  labs(title = \"Comparing Median Income for Homes that Experienced a Blackout or Not\",\n       x = \"Experienced Blackout\",\n       y = \"Median Income\") +\n  theme_minimal()\n\n\nWarning: Removed 10 rows containing missing values (geom_point).\n\n\n\n\n\ncheckout the code\n# loading the summary statistics of our data\nsummary(census_plot_data_blackout)\n\n\n median_income      blackout        \n Min.   : 13886   Length:710        \n 1st Qu.: 43734   Class :character  \n Median : 60634   Mode  :character  \n Mean   : 71435                     \n 3rd Qu.: 89864                     \n Max.   :250001                     \n NA's   :3                          \n\n\ncheckout the code\nsummary(census_plot_data_no_blackout)\n\n\n median_income      blackout        \n Min.   : 24024   Length:402        \n 1st Qu.: 43382   Class :character  \n Median : 57049   Mode  :character  \n Mean   : 67494                     \n 3rd Qu.: 80796                     \n Max.   :250001                     \n NA's   :7                          \n\n\n\n\nconclusion & limitations\nAfter identifying the average median income for homes in the Houston metropolitan area that experienced a blackout during Texas’s 2021 energy crisis, this study showed that average median income for homes that experienced a blackout was $71,435 and was higher for the average median income for homes that didn’t experience a blackout at $64,494.\nHowever, this study didn’t account for the percentage of homes that fell in lower median income tracks versus the percentage of homes that fell in higher median income census tracks and thus weights all census tracks equally upon calculating the average median income. Further investigations could also group census tracks by income level and identify the percentage of impacted vs non-impacted homes for each income grouping to determine if lower median income levels were disproportionately affected compared to higher median income levels.\nIn addition, the study excluded homes that were 200 meters from highways. This could disproportionately exclude homes with lower median incomes. In addition, this study only looked at median income factors within census tracts and not other socioeconomic factors or medical vulnerability factors.\nThis goal of this investigation was to become more familiar with spatial data. The results and findings of this investigation are not final and should not be cited without additional investigations. Overall, I hope this blog post was helpful in learning how different packages and functions can be used for working with spatial data.\n\n\n\n\n\nFootnotes\n\n\n(online?){flores2022, author = {Flores, N.M., McBrien, H., Do, V. et al.}, title = {The 2021 Texas Power Crisis: distribution, duration, and disparities.}, date = {2022-08-13}, url = {https://doi.org/10.1038/s41370-022-00462-5}, langid = {en} }↩︎\n\nCitationBibTeX citation:@online{mccamy2022,\n  author = {Colleen McCamy},\n  title = {Lights {Out} in {Texas}},\n  date = {2022-11-15},\n  url = {https://colleenmccamy.github.io/2022-10-24-first-blog-test},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nColleen McCamy. 2022. “Lights Out in Texas.” November 15,\n2022. https://colleenmccamy.github.io/2022-10-24-first-blog-test."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "projects",
    "section": "",
    "text": "This page is a work-in-progress and will be updated as more projects come in."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "colleen mccamy",
    "section": "",
    "text": "education\nMaster of Environmental Data Science, 2023 | University of California, Santa Barbara, Santa Barbara, CA\nBA in Environmental Science, 2018 | University of California, Santa Barbara, Santa Barbara, CA\n\n\n\nexperience\nSilicon Valley Clean Energy\n\nDigital Engagement & Data Visualization Specialist (2022 - present)\nMarketing Specialist (2021-2022)\nCommunity Outreach Specialist (2019-2021)\nClimate Corps Fellow (2018-2019)\n\nUC Carbon Neutrality Initiative\n\nCarbon Neutrality Fellow (2015-2017)"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "about",
    "section": "",
    "text": "I received a Bachelor of Arts in Environmental Studies at the University of California, Santa Barbara and held many jobs which helped her navigate the wide range of paths within the environmental field – such as working at a research lab focused on kelp forest ecosystems, at a UC Natural Reserve in the Eastern Sierra Nevada’s, and with hands-on environmental education.\n\n\nHowever, it was my fellowship with the UC Carbon Neutrality Initiative that introduced me to the world of clean energy and the impact she can make in the field. After graduating, I followed my interest and joined the community choice energy agency, Silicon Valley Clean Energy. In my role, I managed and developed digital tools to help communities electrify and take advantage of their local, clean electricity.\nI aim to build creative data visualizations that connect communities to energy insights and help guide policies, encourage action, and amplify equitable solutions."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "blog",
    "section": "",
    "text": "data science\n\n\ngraphics\n\n\n\nHighlighting biases entangled with environmental applications of AI\n\n\n\nColleen McCamy\n\n\nDec 3, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndata science\n\n\nR\n\n\nmodeling\n\n\ntime series\n\n\n\nExploring energy demand data in light of recent electricity policy changes\n\n\n\nColleen McCamy\n\n\nDec 3, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndata science\n\n\nR\n\n\nspatial-analysis\n\n\n\nIdentifying homes affected by the Texas energy crisis\n\n\n\nColleen McCamy\n\n\nNov 15, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndata science\n\n\n\nWelcome to my blog!\n\n\n\nColleen McCamy\n\n\nNov 12, 2022\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2022-12-03-tou-policy-analysis/index.html",
    "href": "posts/2022-12-03-tou-policy-analysis/index.html",
    "title": "Time-of-Use Electricity Rate Analysis",
    "section": "",
    "text": "the question\n\nDid the Time-of-Use electricity rate transition have an effect on energy demand in the greater San Diego region?\n\n\n\nintroduction\n\nCalifornia has ambitious clean energy and decarbonization goals. 1 To achieve these goals, the state will need to increase its current electricity grid capacity by about three times.2 Time-of-use electricity rates are a valuable strategy to help reduce the investment needed for expanding grid capacity and can help maximize the use of renewable resources.3\n\nThe Time-of-Use (TOU) energy rate referenced in this analysis establishes lower electricity prices for times when there is more renewable energy supply available and helps to encourage electricity use when generation is cleanest and lowest-cost. Additionally, energy rates are higher during the evening peak to help promote less energy use during times when renewable energy supply decreases and grid operators need to ramp up generation from fossil-fuel based power plants. For the default residential rate which is the assumed TOU rate in this analysis, the higher cost hours or “peak hours” are from 4:00 - 9:00 p.m. 4\nWhile many utilities have piloted time-varying rates, the recent California TOU transition was the biggest test of time-based rates yet - automatically switching over 20 million electricity consumers to a TOU rate. 5 While there has been some initial research and analysis on time-based rates in electricity markets, there are limited results on mass-market transitions and how time-based rates affect total electricity consumption.\nAnswering the question, ‘did the Time-of-Use electricity rate transition have an effect on peak energy demand in the San Diego region?,’ can help provide insight on the policy’s effect and spur further investigations on time-based rates throughout the state of California and beyond.\n\n\nthe data\n\nElectricity Demand Data\nElectricity demand data used in this analysis are publicly available and provided by the US Energy Information Administration. The data were downloaded via the API dashboard 6 and were selected to include hourly electricity demand in megawatt hours (MWh) for the dates of July 1, 2018 to July 31, 2022, in the local time zone (Pacific), and for the San Diego Gas and Electric (SDGE) subregion. SDGE serves 3.7 million people through 1.5 million electric meters. Their service territory covers 4,100 square miles in San Diego and southern Orange counties and the energy demand data are an aggregate hourly electricity demand from all customers with SDGE service. 7\n\n\n\n\n\n\ncheckout the code\n#loading the necessary libraries\nlibrary(dplyr)\nlibrary(tidyverse)\nlibrary(here)\nlibrary(readr)\nlibrary(gt)\nlibrary(tufte)\nlibrary(feasts)\nlibrary(janitor)\nlibrary(lubridate)\nlibrary(broom)\nlibrary(tsibble)\nlibrary(ggpubr)\nlibrary(ggiraph)\nlibrary(ggiraphExtra)\nlibrary(sjPlot)\nlibrary(ggcorrplot)\nlibrary(car)\nlibrary(modelr)\n\n# setting my root directory\nrootdir <- (\"/Users/colleenmccamy/Documents/MEDS/EDS_222_Stats/final_project\")\n\n# reading in the data\neia_data_raw <- read_csv(paste0(rootdir, \"/data/eia_data.csv\"))\n\n# cleaning the data to be the two variables of interest\neia_df <- eia_data_raw |> \n  select(date, hourly_energy_mwh) |> \n  na.omit()\n  \n# creating a time series dataframe\neia_ts <- eia_df |> \n  as_tsibble()\n\n\nTemperature Data\nIn California, peak electricity demand and temperature is highly correlated. 8 As this investigation looks into energy demand, temperature data was added to the analysis. The temperature data used in the following analysis are publicly available through the NOWData Online Weather Data portal from the National Weather Service, a branch of the National Oceanic and Atmospheric Administration.9 The temperature data includes an average of daily maximum, minimum, and average temperature from numerous weather stations throughout San Diego County in degrees Fahrenheit (F). This analysis uses maximum daily temperature at the same temporal scale as the energy demand data.\n\n\n\nSince the temperature data is an aggregate of multiple stations throughout San Diego County, this can cause bias as the SDGE service territory covers multiple different climate zones 10 which may not be accurately represented within the aggregate average temperature for the stations. Also, the weather stations are more heavily concentrated towards the coast which may be a source of bias towards cooler, more moderate in the data used.\n\n\ncheckout the code\n# loading in the temperature data\ntemp_data <- read_csv(paste0(rootdir, \"/data/sd_temp_data.csv\"))\n\n# wrangling the data\ntemp_data <- temp_data |> \n  mutate(temp_max = as.numeric(temp_max)) |> \n  mutate(temp_min = as.numeric(temp_min)) |> \n  mutate(temp_avg = as.numeric(temp_avg)) |> \n  mutate(temp_dept = as.numeric(temp_dept)) |> \n  mutate(date = lubridate::mdy(Date)) |> \n  select(!Date)\n\n# restructuring the eia data to merge the dataset with the temperature data by date\neia_data <- eia_df |> \n  mutate(time = (date)) |> \n  mutate(date = as.Date(date))\neia_data$time <- format(eia_data$time, format = \"%H:%M:%S\")\n\n# merging the data into one dataframe\nenergy_temp_df <- left_join(x = eia_data,\n                            y = temp_data,\n                            by = \"date\")\n\n\nExploratory Data Visualizations\nThe following figures outline the sum of peak daily energy demand hours and the daily max temperature from our datasets.\n\n\ncheckout the code\n# exploring the data by plotting energy demand throughout time\nenergy_demand_plot <- ggplot(data = eia_df,\n       aes(x = date, \n           y = hourly_energy_mwh)) +\n  geom_line(col = \"#b52b8c\") +\n  labs(title = \"Hourly Energy Demand (MWh)\",\n       x = \"Date\",\n       y = \"MWh\") +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5))\n\n# exploring the data by plotting maximum temperature throughout time\nmax_temp_plot <- ggplot(temp_data, aes(x = date, y = temp_max)) + \n  geom_line(col = \"#52796f\") +\n  labs(title = \"Maximum Temperature per day (°F)\",\n       x = \"Date\",\n       y = \"Max Temperature (°F)\") +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5))\n\n# creating dataframe for tou peak horus\ntou_peak_hours_df <- energy_temp_df |> \n  filter(time >= 16 & time <= 21)\n\n# grouping it for daily peak hours to plot with daily maximum temperature\ndaily_peak_hrs_df <- tou_peak_hours_df |> \n   group_by(date) |> \n   summarize(daily_energy_mwh = sum(hourly_energy_mwh))\n\n# plotting daily peak energy demand with daily max temperatures\npeak_demand_plot <- ggplot(data = daily_peak_hrs_df,\n       aes(x = date, \n           y = daily_energy_mwh)) +\n  geom_line(col = \"#b52b8c\") +\n  labs(title = \"Hourly Energy Demand (MWh)\",\n       x = \"Date\",\n       y = \"MWh\") +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5))\n\n# plotting along with daily temperature\nggarrange(peak_demand_plot, max_temp_plot,\n                    ncol = 2, nrow = 1)\n\n\n\n\n\ncheckout the code\n# restructuring the eia data to merge the dataset with the temperature data by date\neia_data <- eia_df |> \n  mutate(time = (date)) |> \n  mutate(date = as.Date(date))\neia_data$time <- format(eia_data$time, format = \"%H:%M:%S\")\n\n# merging the data into one dataframe\nenergy_temp_df <- left_join(x = eia_data,\n                            y = temp_data,\n                            by = \"date\")\n\n\n\n\nanalysis\nA multi-linear regression model and time series decomposition analysis can help answer the question at hand. Prior to conducting the linear model, I also used summary statistics to a establish cutoff point in creating temperature as a dichotomous variable.\n\nLinear Model\nTo investigate if the implementation of the TOU policy had an effect on energy demand, I used a multiple linear regression model as other factors also have an effect on electricity demand such as the temperature and hour of the day. The equation for this model is: \\[hwy_i =\\beta_{0}+\\beta_{1} \\cdot TOUPolicy_i +\\beta_{2} \\cdot \\text HotDay_i+ \\beta_{3} \\cdot \\text PeakHour_i +\\varepsilon_i\\]\nThe ‘TOUPolicy’ predictor (‘tou_policy’ in the results) is a dichotomous variable indicating if the TOU Policy was in effect or not. The ‘PeakHour’ predictor is also a dichotomous variable which indicates whether or not the hour of the day was during peak times from 4:00 - 9:00 p.m for the TOU pricing.\nLastly, the ‘HotDay’ variable (‘hot_day’ in the results) is a dichotomous variable indicating if the maximum temperature for the San Diego region was equal to or greater than 80°F or below 80°F. This cutoff temperature was determined by looking at the mean and standard deviation of the maximum temperature in San Diego during the time of interest. The average maximum temperature was about 72°F and the standard deviation about 7°F. Thus, 80°F was determined to be a ‘hot day’ for exploring the effect of heat and hourly electricity demand in the model.\n\n\ncheckout the code\n### ---- Determining a \"Hot Day\" ---- \n\n# determining the mean and standard deviation for the time period of interest\nmean_max_temp <- mean(energy_temp_df$temp_max, na.rm = TRUE)\nsd_max_temp <- sd(energy_temp_df$temp_max, na.rm = TRUE)\n\n# preparing the data to plot\nbox_data <- as_tibble(energy_temp_df$temp_max)\n\n### ---- Adding a 'Hot Day' Indicator in the Dataframe ---- \ntemp_demand_daily <- energy_temp_df |> \n  group_by(date) |> \n  summarize(daily_energy_mwh = sum(hourly_energy_mwh)) |> \n  left_join(temp_data, by = \"date\") |> \n  mutate(hot_day = case_when(\n    (temp_max >= 80) ~ 1,\n    (temp_max <= 79) ~ 0))\n\n### ----- Adding TOU Policy and Peak Hours to Dataframe -----\n\n# adding a year separate year column in the dataframe\nenergy_temp_df <- energy_temp_df |> \n  mutate(year = date)\n\nenergy_temp_df$year <- format(energy_temp_df$year, format = \"%Y\") \n\n# using variables to create dichotomous predictors\nenergy_temp_df <- energy_temp_df |> \n  mutate(tou_policy = case_when(\n    (year > 2020) ~ 1,\n    (year <= 2020) ~ 0)) |> \n  mutate(time = as_datetime(time, format = \"%H:%M:%S\")) |> \n  mutate(time = lubridate::hour(time)) |> \n  mutate(tou_policy = case_when(\n    (year > 2020) ~ 1,\n    (year <= 2020) ~ 0)) |> \n  mutate(peak_hours = case_when(\n    (time < 16) ~ 0,\n    (time >= 16 & time <= 21 ) ~ 1,\n    (time > 21) ~0)) |> \n  mutate(hot_day = case_when(\n    (temp_max >= 80) ~ 1,\n    (temp_max <= 79) ~ 0))\n\n#### ----- Linear Regression on Hourly Energy Demand ---- ###\nmodel_tou_peak_demand <- lm(formula = hourly_energy_mwh ~ \n                              tou_policy + \n                              peak_hours +\n                              hot_day, \n                            data = energy_temp_df)\n\n\nThe previous regression model predicts energy demand given that each predictor doesn’t influence the relationship between the energy demand and other variables. However, thinking about the goal of TOU policy, we can add an interaction model to the linear regression to see if the effect of peak hours is affected by the TOU policy implementation. I would predict that there would be a greater energy demand decrease after the TOU policy was implemented than before during the peak hours. The new interaction model equation is: \\[hwy_i =\\beta_{0}+\\beta_{1} \\cdot TOUPolicy_i +\\beta_{2} \\cdot \\text HotDay_i+ \\beta_{3} \\cdot \\text PeakHour_i + \\beta_{3} \\cdot \\text PeakHour_i \\cdot TOUPolicy_i  + \\varepsilon_i\\]\n\n\ncheckout the code\nmodel_int_tou_peak_demand <- lm(formula = hourly_energy_mwh ~ \n                              tou_policy + \n                              peak_hours +\n                              hot_day + \n                              peak_hours * tou_policy,\n                            data = energy_temp_df)\n\nsummary(model_int_tou_peak_demand)\n\n\n\n\nTime Series Analysis\nDiving deeper into additional affects on energy demand, I conducted a classical decomposition analysis to investigate if seasonality or any overall trends influence hourly electricity demand within the time frame we are interested in.\n\n\ncheckout the code\nx = seq(from = ymd('2018-07-1'), \n        length.out = 1481,\n        by='day')\n\n# preparing the dataframe for the time series\ndecom_df <- energy_temp_df |>\n  group_by(date) |>\n  summarize(daily_energy_mwh = sum(hourly_energy_mwh)) |> \n  mutate(index = x)\n\ndecom_ts <- as_tsibble(decom_df, index = index)\n\ndecom_plot_annual <- model(decom_ts, \n                    classical_decomposition(daily_energy_mwh ~ \n                                              season(365), \n                                            type = \"additive\")) |> \n  components() |> \n  autoplot(col = \"#3d405b\") +\n  theme_minimal() +\n  labs(title = \"Classical Decomposition Model\",\n       subtitle = \"Seasonality defined as 365 days\",\n       x = \"Date\",\n       caption = \"Figure 4\")\n\ndecom_plot_monthly <- model(decom_ts, \n                    classical_decomposition(daily_energy_mwh ~ \n                                              season(30), \n                                            type = \"additive\")) |> \n  components() |> \n  autoplot(col = \"#3d405b\") +\n  theme_minimal() +\n  labs(title = \"Classical Decomposition Model\",\n       subtitle = \"Seasonality defined as 30 days\",\n       x = \"Date\",\n       caption = \"Figure 3\")\n\n\n\n\n\nresults\nMultiple Linear Regression:\nFrom our model, we can interpret all of the predictors used in the regression are statistically significant in predicting hourly electricity demand at a significance level of 0.001 as they all had a p-value of 2 x e-16. The model indicates that when the daily maximum temperature is below 80°F and we are looking at non-peak hours prior to the Time-of-Use implementation in 2020, the average hourly electricity demand is about 2,164 MWh for the SDGE’s service territory. In addition, we expect to see on average a decrease in hourly electricity demand by about 108 MWh for years after the Time-of-Use energy policy was implemented, holding all other predictors constant. For days in which the maximum temperature is above 80°F, the model predicts that the average hourly electricity demand increases by about 409 MWh holding all other predictors constant.\nInterestingly, the model predicts that the average hourly electricity demand decreases by about 360 MWh holding all other predictors constant. At first thought, we may expect too see hourly electricity demand to increase during peak times as these are times in which the TOU electricity policy calls out as times with higher energy demand. However, in this analysis we didn’t look at the amount of renewable electricity available on the grid. Thus, overall energy demand may be lower during peak times but it is possible the percent of average hourly electricity demand in relation to hourly renewable electricity available on the grid may be higher during peak times than non-peak times.\nTable 1 in the supporting figures section highlights these estimates, p-value and confidence interval for each of the predictors and intercept and the following equation for the linear regression model is:\n\\[Hourly EnergyDemand = 2164 - {108} \\cdot TOUPolicy_i - {360} \\cdot \\text HotDay_i+ {409} \\cdot \\text PeakHour_i +\\varepsilon_i\\]\nMultiple Linear Regression with the Interaction Model:\nThis interaction model tells us that the average change in hourly electricity demand during peak hours after the TOU policy was implemented compared to hourly energy demand during peak hours before the TOU policy was implemented is -98 MWh. This estimate is statistically significant at a significance level of 0.001 (p-value of 2 x e-16). The graph (Figure 2) below of the linear model with the interaction added indicates that the relationship between energy demand and peak hours varies based on the implementation of the TOU policy as the slope of the lines are different.\n\n\ncheckout the code\nggPredict(model_int_tou_peak_demand, \n          jitter = TRUE, \n          interactive = TRUE)\n\n\n\n\n\n\ncheckout the code\nprint(\"Figure 2\")\n\n\n[1] \"Figure 2\"\n\n\nThe adjusted R2 values also increased slightly (from 0.213 to 0.214) when adding the interaction model. This demonstrates that the interaction model just slightly increases model fit.\nHowever, the adjusted R2 value indicates that only about 21% of hourly electricity demand is explained by the model. With this information, we can hypothesis that there are other factors that affect hourly electricity demand not included within this model. Therefore, we can state that the time-of-use policy implementation had a statically significant decrease in hourly electricity demand (at a significance level of 0.001) but overall hourly electricty demand is more greater affected by other predictors.\nTime Series Analysis - Classical Decomposition:\nTo better understand the other factors that influence hourly energy demand, we can look at classical decomposition graphs for our time series data for both yearly and monthly seasonality (Figure 3 and 4).\n\n\ncheckout the code\nggarrange(decom_plot_monthly, decom_plot_annual,\n                    ncol = 1, nrow = 2)\n\n\n\n\n\nLooking at the graphs there doesn’t appear to be evidence of a long-run trend in hourly energy demand over the time period analyzed as the trend seems to be mostly constant when seasonality is defined as 30 days and 365 days. It also appears that seasonality may be important in driving overall variation in electricity demand when seasonality is defined as 365 days since the gray bar is closer in height to gray bar for the overall time series graph. Anecdotally, this is intuitive as we can predict that the variance in hourly electricity could be affected by the month since month of the year and temperature are correlated and energy demand and temperature are also correlated. However, when seasonality is defined as 30 days, the seasonal effect appears to be not as important in the driving overall variation in electricity demand.\n\n\ndiscussion & conclusion\nWith this initial analysis we can conclude that the time-of-use energy rate policy had an effect on hourly electricity demand in SDGE’s service territory, and had a greater effect during peak hours. However, we can also see how other factors not addressed in the model account for more of the variation in hourly electricity demand.\nAdditional research and analysis should be done to determine how the time-of-use policy affects electricity demand in relation to the other factors that weren’t addressed in this analysis. This can include dividing electricity demand by customer type or rate class, as this analysis just uses an aggregated energy demand for all customers. To expand further on this, this analysis was conducted with the assumption that all SDGE customers transitioned to a Time-of-Use energy rate that had peak hours from 4:00 - 9:00 p.m. (TOU-C rate) and were transitioned all at the same time. We know that this is not the case as it was a rolling transition throughout the year 2020 and not all customers were transitioned to the TOU-C rate. Conducting this analysis with a more accurate indication of TOU implementation per customer is needed. However, this type information is not publicly available by utilities as it can include confidential customer information.\nIn addition, this analysis is also spatially limited for SDGE’s service area additional research can expand this investigation statewide. Lastly, this investigation just looks at electricity demand. However, as noted previously a key part of time-of-use policy is to reduce energy use when demand is high and renewable supply is low. Instead of solely using hourly electricity demand as the outcome variable, future research can look into how the TOU policy affects energy demand when renewable supply is low.\nGiven additional analysis is conducted, this information can be used to inform policy makers, energy providers (load-serving entities) and grid operators about the effectiveness of the TOU policy and and how the policy can continue to support California’s clean energy goals.\n\n\nsupporting figures & links\nTo see the full repository, check out the project on Github at:\nhttps://github.com/colleenmccamy/tou-energy-analysis\nTable 1:\n\n\ncheckout the code\ntab_model(model_tou_peak_demand,\n          pred.labels = c(\"Intercept\", \n                          \"TOU Policy In Effect\", \n                          \"During Peak Hours\", \n                          \"Max. Temp above 80 (°F)\"),\n          dv.labels = c(\"Hourly Electricity Demand (MWh)\"),\n          string.ci = \"Conf. Int (95%)\",\n          string.p = \"P-value\",\n          title = \"Table 1. Linear Model Results for Predictors on Hourly Electricity Demand\",\n          digits = 2)\n\n\n\n\nTable 1. Linear Model Results for Predictors on Hourly Electricity Demand\n\n \nHourly Electricity Demand (MWh)\n\n\nPredictors\nEstimates\nConf. Int (95%)\nP-value\n\n\nIntercept\n2164.03\n2157.53 – 2170.52\n<0.001\n\n\nTOU Policy In Effect\n-107.82\n-116.89 – -98.75\n<0.001\n\n\nDuring Peak Hours\n-360.18\n-370.32 – -350.03\n<0.001\n\n\nMax. Temp above 80 (°F)\n409.10\n396.47 – 421.74\n<0.001\n\n\nObservations\n36006\n\n\nR2 / R2 adjusted\n0.213 / 0.213\n\n\n\n\n\n\nTable 2:\n\n\ncheckout the code\ntab_model(model_int_tou_peak_demand,\n          pred.labels = c(\"Intercept\", \n                          \"TOU Policy In Effect\", \n                          \"During Peak Hours\", \n                          \"Max. Temp above 80 (°F)\",\n                          \"TOU Policy & Peak Hours\"),\n          dv.labels = c(\"Hourly Electricity Demand (MWh)\"),\n          string.ci = \"Conf. Int (95%)\",\n          string.p = \"P-value\",\n          title = \"Table 2. Linear Model Results for Predictors on Hourly Electricity Demand with an Interaction Addition\",\n          digits = 2)\n\n\n\n\nTable 2. Linear Model Results for Predictors on Hourly Electricity Demand with an Interaction Addition\n\n \nHourly Electricity Demand (MWh)\n\n\nPredictors\nEstimates\nConf. Int (95%)\nP-value\n\n\nIntercept\n2154.55\n2147.77 – 2161.34\n<0.001\n\n\nTOU Policy In Effect\n-83.11\n-93.56 – -72.66\n<0.001\n\n\nDuring Peak Hours\n-322.25\n-335.15 – -309.35\n<0.001\n\n\nMax. Temp above 80 (°F)\n409.12\n396.50 – 421.73\n<0.001\n\n\nTOU Policy & Peak Hours\n-98.96\n-119.80 – -78.12\n<0.001\n\n\nObservations\n36006\n\n\nR2 / R2 adjusted\n0.215 / 0.215\n\n\n\n\n\n\nQQ Plot for hourly energy demand residuals: This supports that a linear model is an appropriate method in conducting our analysis as the residual from the model predictions appear to be mainly normal.\nFigure 5:\n\n\ncheckout the code\naug <- energy_temp_df |>  \n  add_predictions(model_int_tou_peak_demand) |> \n  mutate(residuals_energy = hourly_energy_mwh - pred)\n\nqqPlot(aug$residuals_energy) \n\n\n\n\n\n[1] 16604 16605\n\n\nBox plot for exploring Maximum Temperature Data:\nFigure 6:\n\n\ncheckout the code\n# plotting the mean and standard deviation\ntemp_box <- ggplot(box_data) +\n  geom_boxplot(aes(x = value), col = \"#300e2e\",\n               fill = \"#8a6d88\") +\n  labs(x = \"Maximum Daily Temperature (°F)\") +\n  theme_minimal()\n\ntemp_box\n\n\n\n\n\n\n\nreferences\n\n\n\n\n\nFootnotes\n\n\nCalifornia, State of. 2022. “California Releases World’s First Plan to Achieve Net Zero Carbon Pollution.” California Governor. November 16, 2022. https://www.gov.ca.gov/2022/11/16/california-releases-worlds-first-plan-to-achieve-net-zero-carbon-pollution/. }↩︎\nCalifornia, State of. 2022. “California Releases World’s First Plan to Achieve Net Zero Carbon Pollution.” California Governor. November 16, 2022. https://www.gov.ca.gov/2022/11/16/california-releases-worlds-first-plan-to-achieve-net-zero-carbon-pollution/. }↩︎\nhttp://www.caiso.com/documents/matchingtimeofuseperiodswithgridconditions-fastfacts.pdf }↩︎\n“Time of Use.” n.d. SVCE (blog). Accessed December 3, 2022. https://svcleanenergy.org/time-of-use/. }↩︎\n“California Utilities Prep Nation’s Biggest Time-of-Use Rate Rollout.” n.d. Utility Dive. Accessed December 3, 2022. https://www.utilitydive.com/news/california-utilities-prep-nations-biggest-time-of-use-rate-roll-out/543402/. }↩︎\n“API Dashboard - U.S. Energy Information Administration (EIA).” n.d. Accessed December 3, 2022. https://www.eia.gov/opendata/browser/electricity/rto/region-sub-ba-data. }↩︎\n“Our Company | San Diego Gas & Electric.” n.d. Accessed December 3, 2022. https://www.sdge.com/more-information/our-company. }↩︎\nMiller, Norman L., Katharine Hayhoe, Jiming Jin, and Maximilian Auffhammer. 2008. “Climate, Extreme Heat, and Electricity Demand in California.” Journal of Applied Meteorology and Climatology 47 (6): 1834–44. https://doi.org/10.1175/2007JAMC1480.1. }↩︎\nUS Department of Commerce, NOAA. n.d. “Climate.” NOAA’s National Weather Service. Accessed December 3, 2022. https://www.weather.gov/wrh/Climate?wfo=sgx. }↩︎\n“Baseline Allowance Calculator | San Diego Gas & Electric.” n.d. Accessed December 4, 2022. https://webarchive.sdge.com/baseline-allowance-calculator.}↩︎\n\nCitationBibTeX citation:@online{mccamy2022,\n  author = {Colleen McCamy},\n  title = {Time-of-Use {Electricity} {Rate} {Analysis}},\n  date = {2022-12-03},\n  url = {https://colleenmccamy.github.io/2022-12-03-tou-policy-analysis},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nColleen McCamy. 2022. “Time-of-Use Electricity Rate\nAnalysis.” December 3, 2022. https://colleenmccamy.github.io/2022-12-03-tou-policy-analysis."
  },
  {
    "objectID": "posts/2022-12-06-ethics-of-ai-in-circles/index.html",
    "href": "posts/2022-12-06-ethics-of-ai-in-circles/index.html",
    "title": "Ethics of AI Illustrated in Circles",
    "section": "",
    "text": "CitationBibTeX citation:@online{mccamy2022,\n  author = {Colleen McCamy},\n  title = {Ethics of {AI} {Illustrated} in {Circles}},\n  date = {2022-12-03},\n  url = {https://colleenmccamy.github.io/2022-12-03-tou-policy-analysis},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nColleen McCamy. 2022. “Ethics of AI Illustrated in\nCircles.” December 3, 2022. https://colleenmccamy.github.io/2022-12-03-tou-policy-analysis."
  }
]