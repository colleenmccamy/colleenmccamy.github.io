[
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "resume",
    "section": "",
    "text": "This webpage is undergoing construction. Resume will be updated shortly."
  },
  {
    "objectID": "posts/2022-10-24-first-blog-test/index.html",
    "href": "posts/2022-10-24-first-blog-test/index.html",
    "title": "First blog post",
    "section": "",
    "text": "CitationBibTeX citation:@online{mccamy2022,\n  author = {Colleen McCamy},\n  title = {First Blog Post},\n  date = {2022-11-12},\n  url = {https://colleenmccamy.github.io/2022-11-12-first-blog-post},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nColleen McCamy. 2022. “First Blog Post.” November 12, 2022.\nhttps://colleenmccamy.github.io/2022-11-12-first-blog-post."
  },
  {
    "objectID": "posts/2022-11-15-lights-out-in-texas/index.html",
    "href": "posts/2022-11-15-lights-out-in-texas/index.html",
    "title": "Lights Out in Texas",
    "section": "",
    "text": "time to code\nThis section illustrates the code executed to explore our questions.\nHeading to the library\nGet your library card ready because it is time to load these packages.\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nLinking to GEOS 3.10.2, GDAL 3.4.2, PROJ 8.2.1; sf_use_s2() is TRUE\n\n\nLoading required package: abind\n\n\nLoading required package: sp\n\n\n\nAttaching package: 'raster'\n\n\nThe following object is masked from 'package:dplyr':\n\n    select\n\n\nterra 1.6.17\n\n\nStep 1: Diving into the Data\nWhat would be a data science project without the data? The following code block uses SQL and the stars package to load in the data needed for the project.\nFor our raster data we used NASA’s Worldview data for February 7, 2021 (before the power outage) and Februray 16, 2021 (during the power outage) to visualize the extent of the power outage in the Houston area. These days were selected as other days during the time frame had too much cloud cover to be useful.\nVIIRS data is distributed through NASA’s Level-1 and Atmospheric Archive & Distribution System Distributed Active Archive Center (LAADS DAAC). Data was downloaded and prepped in advance to this assignment.\n\n#reading in the NASA raster files\nnl_feb07_t1<- read_stars('/Users/colleenmccamy/Documents/MEDS/EDS_223_Spatial_Data/data/assignment3/VNP46A1/VNP46A1.A2021038.h08v05.001.2021039064328.tif')\n\nnl_feb07_t2 <- read_stars('/Users/colleenmccamy/Documents/MEDS/EDS_223_Spatial_Data/data/assignment3/VNP46A1/VNP46A1.A2021038.h08v06.001.2021039064329.tif')\n\nnl_feb16_t1 <-read_stars('/Users/colleenmccamy/Documents/MEDS/EDS_223_Spatial_Data/data/assignment3/VNP46A1/VNP46A1.A2021047.h08v05.001.2021048091106.tif')\n\nnl_feb16_t2 <- read_stars('/Users/colleenmccamy/Documents/MEDS/EDS_223_Spatial_Data/data/assignment3/VNP46A1/VNP46A1.A2021047.h08v06.001.2021048091105.tif')\n\nNext up we have data from roads. To minimize accounting for light from major highways systems, we used publicly available geographic data from OpenStreetMap (OSM) through Geofabrik’s download sites. This data were downloaded and prepped in advance to contain a subset of highway and raods that intersect with the Houston metropolitan area. We also used data from Geofabrik’s download sites for information on houses in the Houston metropolitan area.\n\n## ---- Highway Data\n# reading in highway data using SQL query\nquery <- \"SELECT * FROM gis_osm_roads_free_1 WHERE fclass='motorway'\"\n\n# reading in the highways data with st_read()\nhighways <- st_read(\"/Users/colleenmccamy/Documents/MEDS/EDS_223_Spatial_Data/data/assignment3/gis_osm_roads_free_1.gpkg\", query = query)\n\n\n## ---- Houses Data\n# defining the query for the houses\nquery_houses <- \"SELECT * FROM gis_osm_buildings_a_free_1 WHERE (type IS NULL AND name IS NULL) OR type in ('residential', 'apartments', 'house', 'static_caravan', 'detached')\"\n\n# reading in the highways data with st_read()\nhouses <- st_read(\"/Users/colleenmccamy/Documents/MEDS/EDS_223_Spatial_Data/data/assignment3/gis_osm_buildings_a_free_1.gpkg\", query = query_houses)\n\nLastly, we use data from the U.S. Census Bureau’s American Community Survey for census tracts in 2019 from an ArcGIS file geodatabase. The metadata for each layer is available at ACS metadata and this data was downloaded in advance to this investigation.\n\n#reading in geometry data and selecting for the layer containing the geometry\ncensus_geom <- st_read(\"/Users/colleenmccamy/Documents/MEDS/EDS_223_Spatial_Data/data/assignment3/ACS_2019_5YR_TRACT_48_TEXAS.gdb\", layer = \"ACS_2019_5YR_TRACT_48_TEXAS\")\n\n#reading in income data\nincome_median <- st_read(\"/Users/colleenmccamy/Documents/MEDS/EDS_223_Spatial_Data/data/assignment3/ACS_2019_5YR_TRACT_48_TEXAS.gdb\", layer = \"X19_INCOME\")\n\nStep 2: Creating a blackout mask\nThe following code outlines how I created a blackout mask for the Houston area that I could use for identifying impacted homes. We operated under the assumption that any location that experienced a drop of more than 200 nW cm-2sr-1 experienced a blackout.\n\n# combing the data into single stars objects for each day\nfeb16_tile <- st_mosaic(nl_feb16_t1, nl_feb16_t2)\nfeb07_tile <- st_mosaic(nl_feb07_t1, nl_feb07_t2)\n\n# adding an indicator of the attributes in the data\nfeb_16_tile_names = setNames(feb16_tile, \"light_16\")\nfeb_07_tile_names = setNames(feb07_tile, \"light_07\")\n\n# matrix alegbra to calculate the difference light difference between the two dates \nblackout_dif <- feb_07_tile_names - feb_16_tile_names\n\n# #filtering for the differences of a drop less that 200 nW cm-2sr-1 as NA\nblackout_mask <- cut(blackout_dif, c(200, Inf), labels = \"outage\")\n\n# vectorizing the blackout mask and fixing any invalid geometries\nblackout_mask_v <- st_as_sf(blackout_mask) |> \n  st_make_valid()\n\n# creating a polygon of Houston's coordinates \nhou_border <- st_polygon(list(rbind(c(-96.5,29), c(-96.5,30.5), c(-94.5, 30.5), c(-94.5,29), c(-96.5,29))))\n\n# converting to an sf object and identifying the coordinate reference system\nhou_border_sf <- st_sfc(hou_border, crs = 'EPSG:4326')\n\n# cropping the blackout mask with the Houston polygon\nhou_outage_mask_v <- blackout_mask_v[hou_border_sf, ,]\n\n# reprojectting the cropped object to a new crs and converting it as an sf object\nhou_outage_mask_v_3083 <- st_transform(hou_outage_mask_v, crs = 'EPSG:3083')\noutage_mask_clean <- st_as_sf(hou_outage_mask_v_3083)\n\nStep 3: Excluding highway data\nTo exclude light from highways, we created a buffer of 200 meters and kept areas in our blackout mask that were greater than 200 meters away from a highway.\n\n# selecting the highway geometry data\nhighways_geom <- highways$geom\n\n# transforming the highway geometries to the consistent crs\nhighways_geom <- st_transform(highways_geom, crs = 'EPSG:3083')\n\n# creating a buffer zone for highways geometry data of 200 meters\nhighway_buffer <- st_buffer(x = highways_geom, dist = 200)\nhighway_buffer <- st_transform(highway_buffer, crs = 'EPSG:3083')\n\n# combining the geometries into one and creating a mask that excludes the highway data\nhighway_buffer <- st_union(highway_buffer, by_feature = FALSE)\nmask_hou_highway <- outage_mask_clean[highway_buffer, , op = st_disjoint]\n\nStep 4: Identifying Impacted Homes\nTo find the number of impacted homes, the code below outlines how I used the new blackout mask with the highway data to identify homes most likely impacted by the power outage.\n\n# transforming houses data to be usable\nhouses <- st_transform(houses, crs = 'EPSG:3083')\nhouses_st <- st_as_sf(houses)\n\n# filtering the houses data with the blackout mask\noutage_houses <- houses_st[mask_hou_highway, drop = FALSE]\n\n# identifying how many homes were affected\nprint(paste0(\"There were \", nrow(outage_houses), \" homes affected by the power outage on Feburary 16, 2021.\"))\n\nStep 4: Investigating Socioeconomic factors\nNow that we have information on the houses that were affected we can match this with the socioeconomic census tract information and determine which census tracts were impacted by the power outage.\n\n# transforming the census data to be consistent with the crs\ncensus_geom <- st_transform(census_geom, crs = 'EPSG:3083')\n\n#selecting the necessary variables and renaming for clarity\nincome_med_select <- income_median |> \n  dplyr::select(\"GEOID\", \"B19013e1\") |> \n  rename(GEOID_Data = GEOID, median_income = B19013e1)\n\n# changing the income object to a data_frame\nincome_med_select_df <- tibble(income_med_select)\n\n# joining census geometries and median income data\ncensus_data <- left_join(census_geom, \n                         income_med_select, \n                         by = \"GEOID_Data\")\n\n# transforming both objects to the correct crs\ncensus_data <- st_transform(census_data, crs = 'EPSG:4326')\noutage_houses <- st_transform(outage_houses, crs = 'EPSG:4326')\n\n# filtering the census data using the outage houses and adding column indicating that these census tracts were part of a blackout\ncensus_outage <- sf::st_filter(census_data, outage_houses) |> \n  mutate(blackout = 'yes')\n\nStep 5: Comparing the incomes of impacted tracts and unimpacted tracts\nIt is time to visualize our findings. This code breaks down the data wrangling needed for the visualizations and the maps and plots created to compare which census tracts experienced a blackout vs the census tracts that did not experience a blackout.\n\n## --- Wrangling our data for our visualizations -----------\n\n# transforming both objects to the crs 4326 to crop it\ncensus_data <- st_transform(census_data, crs = 'EPSG:4326')\nhou_border_sf <- st_transform(hou_border_sf, crs = 'EPSG:4326')\n\n# cropping the census data with the Houston border for filtering\ncensus_data_hou <- census_data[hou_border_sf, ,] \n\n# transforming census data back to the EPSG:3083 crs\ncensus_data_hou <- st_transform(census_data_hou, crs = 'EPSG:3083')\n\n# selecting necessary columns for houston census data\ncensus_data_hou <- census_data_hou |> \n  dplyr::select(\"NAMELSAD\", \"Shape\", \"median_income\", \"GEOID_Data\")\n\n# selecting necessary columns for outage data by census track\ncensus_outage <- census_outage |> \n  dplyr::select(\"blackout\", \"GEOID_Data\")\ncensus_outage_map <- census_outage |> \n  dplyr::select(\"blackout\")\n\n# converting census outage data to a dataframe in order to join\ncensus_outage_df <- as.data.frame(census_outage)\n\n# joining census outage data and census data for all of Houston\ncensus_map_data <- left_join(census_data_hou, \n                             census_outage_df, \n                             by = \"GEOID_Data\")\n\ncensus_map_data <- census_map_data |> \n  dplyr::select('median_income', 'blackout')\n\n# converting census map data to a dataframe to plot\ncensus_plot_data <- data_frame(census_map_data)\n\n# adding an indicator for homes that didn't experience a blackout\ncensus_plot_data <- census_plot_data |> \n  mutate(blackout = replace(blackout, is.na(blackout), \"no\"))\n\n# creating a data frame for homes that experienced a blackout to plot\ncensus_plot_data_blackout <- census_plot_data |> \n  dplyr::select(\"median_income\", \"blackout\") |> \n  filter(blackout == \"yes\")\n\n\n# creating a data frame for homes that didn't experienced a blackout to plot\ncensus_plot_data_no_blackout <- census_plot_data |> \n  dplyr::select(\"median_income\", \"blackout\") |> \n  filter(blackout == \"no\")\n\n\n## ------- Mapping our data ---------------\n\n# changing the view mode to be interactive\ntmap_mode(\"view\")\n\ntmap mode set to interactive viewing\n\n# mapping median income by census track and identifying outages by dots\ntm_shape(census_map_data) +\n  tm_polygons(col = \"median_income\",\n              palette = c(\"#227c9d\", \n                          \"#17c3b2\", \n                          \"#ffcb77\", \n                          \"#ffe2b3\", \n                          \"#feb3b1\", \n                          \"#fe6d73\"),\n              textNA = \"Missing Income Data\", \n              colorNA = \"#e4ebea\",\n              title = \"Median Income\") +\n  tm_shape(census_outage_map) +\n  tm_dots(shape = 1,\n          title = 'blackout') +\n  tm_layout(main.title = \"Houston Census Data by Income that Experienced A Power Outage\",\n            legend.outside = TRUE,\n            main.title.size = 1\n            )\n\n\n\n\n\n\nThe dots indicate which census tracts were impacted by the blackout.\n\n### ---- Plotting our data --------\n\n# plotting census data that experienced a blackout\nggplot(census_plot_data_blackout, aes(x = median_income)) +\n  geom_histogram(color = \"#3d5a80\",fill = \"#98c1d9\") +\n  labs(title = \"Median Income for Homes that Experienced a Blackout\",\n       x = \"Median Income\",\n       y = \"Count\") +\n  theme_minimal()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 3 rows containing non-finite values (stat_bin).\n\n\n\n\n# plotting census data that didn't experienced a blackout\nggplot(census_plot_data_no_blackout, aes(x = median_income)) +\n  geom_histogram(fill = \"#81b29a\",\n                 color = \"#335c67\") +\n  labs(title = \"Median Income for Homes that Didn't Experience a Blackout\",\n       x = \"Median Income\",\n       y = \"Count\") +\n  theme_minimal()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 7 rows containing non-finite values (stat_bin).\n\n\n\n\n# plotting the comparison data via geom jitter plot\nggplot(census_plot_data, aes(x = blackout, y = median_income)) +\n  geom_jitter(width = 0.1,\n              height = 0,\n              color = \"#248577\",\n              alpha = 0.8) +\n  labs(title = \"Comparing Median Income for Homes that Experienced a Blackout or Not\",\n       x = \"Experienced Blackout\",\n       y = \"Median Income\") +\n  theme_minimal()\n\nWarning: Removed 10 rows containing missing values (geom_point).\n\n\n\n\n# loading the summary statistics of our data\nsummary(census_plot_data_blackout)\n\n median_income      blackout        \n Min.   : 13886   Length:710        \n 1st Qu.: 43734   Class :character  \n Median : 60634   Mode  :character  \n Mean   : 71435                     \n 3rd Qu.: 89864                     \n Max.   :250001                     \n NA's   :3                          \n\nsummary(census_plot_data_no_blackout)\n\n median_income      blackout        \n Min.   : 24024   Length:402        \n 1st Qu.: 43382   Class :character  \n Median : 57049   Mode  :character  \n Mean   : 67494                     \n 3rd Qu.: 80796                     \n Max.   :250001                     \n NA's   :7                          \n\n\n\n\nConclusion & Limitations\nAfter identifying the average median income for homes in the Houston metropolitan area that experienced a blackout during Texas’s 2021 energy crisis, this study showed that average median income for homes that experienced a blackout was $71,435 and was higher for the average median income for homes that didn’t experience a blackout at $64,494.\nHowever, this study didn’t account for the percentage of homes that fell in lower median income tracks versus the percentage of homes that fell in higher median income census tracks and thus weights all census tracks equally upon calculating the average median income. Further investigations could also group census tracks by income level and identify the percentage of impacted vs non-impacted homes for each income grouping to determine if lower median income levels were disproportionately affected compared to higher median income levels.\nIn addition, the study excluded homes that were 200 meters from highways. This could disproportionately exclude homes with lower median incomes. In addition, this study only looked at median income factors within census tracts and not other socioeconomic factors or medical vulnerability factors.\nThis goal of this investigation was to become more familiar with spatial data. The results and findings of this investigation are not final and should not be cited without additional investigations. Overall, I hope this blog post was helpful in learning how different packages and functions can be used for working with spatial data.\n\n\n\n\n\nFootnotes\n\n\n(online?){flores2022, author = {Flores, N.M., McBrien, H., Do, V. et al.}, title = {The 2021 Texas Power Crisis: distribution, duration, and disparities.}, date = {2022-08-13}, url = {https://doi.org/10.1038/s41370-022-00462-5}, langid = {en} }↩︎\n\nCitationBibTeX citation:@online{mccamy2022,\n  author = {Colleen McCamy},\n  title = {Lights {Out} in {Texas}},\n  date = {2022-11-15},\n  url = {https://colleenmccamy.github.io/2022-10-24-first-blog-test},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nColleen McCamy. 2022. “Lights Out in Texas.” November 15,\n2022. https://colleenmccamy.github.io/2022-10-24-first-blog-test."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "projects",
    "section": "",
    "text": "This page is a work-in-progress and will be updated as more projects come in."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "colleen mccamy",
    "section": "",
    "text": "GitHub\n  \n  \n    \n     Email\n  \n\n  \n  \n\nwelcome!\n\n\n\n\nHi there, I am Colleen and glad you to host you on my website. I’m currently pursuing a Master of Environmental Data Science at the Bren School of Environmental Science and Management at the UC Santa Barbara.\n\n\n\n\n\nI am working on unlocking and developing the skills to build creative data visualizations that connect communities to energy insights and help guide policies, encourage action, and amplify equitable solutions. In my spare time, you can find me outside on a trail or playing beach volleyball.\n\n\neducation\nMaster of Environmental Data Science, 2023 | University of California, Santa Barbara, Santa Barbara, CA\nBA in Environmental Science, 2018 | University of California, Santa Barbara, Santa Barbara, CA\n\n\n\nexperience\nSilicon Valley Clean Energy\nSunnyvale, CA (2022-present) - Digital Engagement & Data Visualization Specialist (2021-2022) - Marketing Specialist (2019-2021) - Community Outreach Specialist (2018-2019) - Climate Corps Fellow\nUC Carbon Neutrality Initiative\nSanta Barbara, CA (2015-2017) - Carbon Neutrality Fellow,\n\n\n:::"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "about",
    "section": "",
    "text": "I received a Bachelor of Arts in Environmental Studies at the University of California, Santa Barbara and held many jobs which helped her navigate the wide range of paths within the environmental field – such as working at a research lab focused on kelp forest ecosystems, at a UC Natural Reserve in the Eastern Sierra Nevada’s, and with hands-on environmental education.\n\n\nHowever, it was my fellowship with the UC Carbon Neutrality Initiative that introduced me to the world of clean energy and the impact she can make in the field. After graduating, I followed my interest and joined the community choice energy agency, Silicon Valley Clean Energy. In my role, I managed and developed digital tools to help communities electrify and take advantage of their local, clean electricity.\nI aim to build creative data visualizations that connect communities to energy insights and help guide policies, encourage action, and amplify equitable solutions."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "blog",
    "section": "",
    "text": "data science\n\n\nR\n\n\nspatial-analysis\n\n\n\nExploring energy demand data in light of recent policy changes\n\n\n\nColleen McCamy\n\n\nDec 3, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndata science\n\n\nR\n\n\nspatial-analysis\n\n\n\nIdentifying homes affected by the Texas energy crisis\n\n\n\nColleen McCamy\n\n\nNov 15, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndata science\n\n\n\nWelcome to my blog!\n\n\n\nColleen McCamy\n\n\nNov 12, 2022\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2022-12-03-tou-policy-analysis/index.html",
    "href": "posts/2022-12-03-tou-policy-analysis/index.html",
    "title": "Time-of-Use Energy Analysis",
    "section": "",
    "text": "introduction\nCalifornia has ambitious clean energy and decarbonization goals. 1 To achieve these goals, California will need to increase its current electricity grid capacity by about three times. 2 Time-of-use electricity rates are a value strategy to help reduce the investment needed for expanding grid capacity and can help maximize the use of renewable resources. 3\nThe Time-of-Use (TOU) energy rate referenced in this analysis establishes lower electricity prices for times when there is more renewable energy supply available and helps to encourage electricity use when generation is cleanest and lowest-cost. Additionally, energy rates are higher during the early evening peak to help promote less energy use during times when renewable energy supply decreases and grid operators need to ramp up generation from fossil-fuel based power plants. For the default residential rate, the higher cost hours or “peak hours” are from 4:00 - 9:00 p.m. 4\nWhile many utilities have piloted time-varying rates, the recent California TOU transition was the biggest test of time-based rates yet - automatically switching over 20 million electricity consumers to a TOU rate. 5 While there has been some initial research and analysis on time-based rates in electricity markets, there are limited results on mass-market transitions and how time-based rates affect total electricity consumption.\nAnswering the question, ‘did the Time-of-Use electricity rate transition have an effect on peak energy demand in the San Diego region?,’ can help provide insight to spur further investigations on time-based rates throughout the state of California and beyond.\n\n\nthe data\nEnergy Demand Data\nEnergy demand data used in this analysis are publicly available and provided by the US Energy Information Administration. The data were downloaded via the API dashboard 6 and were selected to include the time frame of July 1, 2018 to July 31, 2022, hourly demand by subregion in megawatt hours (MWh) in the local time zone (Pacific), and the San Diego Gas and Electric (SDGE) subregion. SDGE serves 3.7 million people through 1.5 million electric meters covering 4,100 square miles in San Diego and southern Orange counties. The energy demand data are an aggregate electricity demand from all customers throughout SDGE’s service territory.7\n\n\ncheckout the code\n#loading the necessary libraries\nlibrary(dplyr)\nlibrary(tidyverse)\nlibrary(here)\nlibrary(readr)\nlibrary(gt)\nlibrary(tufte)\nlibrary(feasts)\nlibrary(janitor)\nlibrary(lubridate)\nlibrary(broom)\nlibrary(tsibble)\nlibrary(ggpubr)\nlibrary(ggiraph)\nlibrary(ggiraphExtra)\nlibrary(sjPlot)\nlibrary(ggcorrplot)\nlibrary(car)\nlibrary(modelr)\n\n# setting my root directory\nrootdir <- (\"/Users/colleenmccamy/Documents/MEDS/EDS_222_Stats/final_project\")\n\n# reading in the data\neia_data_raw <- read_csv(paste0(rootdir, \"/data/eia_data.csv\"))\n\n# cleaning the data to be the two variables of interest\neia_df <- eia_data_raw |> \n  select(date, hourly_energy_mwh) |> \n  na.omit()\n  \n# creating a time series dataframe\neia_ts <- eia_df |> \n  as_tsibble()\n\n\nTemperature Data\nIn California, peak electricity demand and temperature is highly correlated. 8 As this investigation looks into energy demand, temperature data was added to the analysis. The temperature data used in the following analysis are publicly available through the NOWData Online Weather Data portal from the National Weather Service, a branch of the National Oceanic and Atmospheric Administration. 9 The temperature data includes an average of daily maximum, minimum, and average temperature from numerous weather stations throughout San Diego County in Fahrenheit. This analysis uses the maximum daily temperature for the same temporal scale as the energy demand data.\nSince the temperature data is an aggregate of multiple stations throughout San Diego County, this can cause bias as the SDGE service territory covers multiple different temperate regions which may not be accurately represented within the average of the stations. Also, the weather stations are more heavily concentrated towards the coast. This could cause bias in the temperature maximum temperatures.\n\n\ncheckout the code\n# loading in the temperature data\ntemp_data <- read_csv(paste0(rootdir, \"/data/sd_temp_data.csv\"))\n\n# wrangling the data\ntemp_data <- temp_data |> \n  mutate(temp_max = as.numeric(temp_max)) |> \n  mutate(temp_min = as.numeric(temp_min)) |> \n  mutate(temp_avg = as.numeric(temp_avg)) |> \n  mutate(temp_dept = as.numeric(temp_dept)) |> \n  mutate(date = lubridate::mdy(Date)) |> \n  select(!Date)\n\n# restructuring the eia data to merge the dataset with the temperature data by date\neia_data <- eia_df |> \n  mutate(time = (date)) |> \n  mutate(date = as.Date(date))\neia_data$time <- format(eia_data$time, format = \"%H:%M:%S\")\n\n# merging the data into one dataframe\nenergy_temp_df <- left_join(x = eia_data,\n                            y = temp_data,\n                            by = \"date\")\n\n\nExploratory Data Visualizations\nThe following figures outline the sum of daily energy demand during peak hours and the daily max temperature.\n\n\ncheckout the code\n# exploring the data by plotting energy demand throughout time\nenergy_demand_plot <- ggplot(data = eia_df,\n       aes(x = date, \n           y = hourly_energy_mwh)) +\n  geom_line(col = \"#b52b8c\") +\n  labs(title = \"Hourly Energy Demand (MWh)\",\n       x = \"Date\",\n       y = \"MWh\") +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5))\n\n# exploring the data by plotting maximum temperature throughout time\nmax_temp_plot <- ggplot(temp_data, aes(x = date, y = temp_max)) + \n  geom_line(col = \"#52796f\") +\n  labs(title = \"Maximum Temperature per day (°F)\",\n       x = \"Date\",\n       y = \"Max Temperature (°F)\") +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5))\n\n# creating dataframe for tou peak horus\ntou_peak_hours_df <- energy_temp_df |> \n  filter(time >= 16 & time <= 21)\n\n# grouping it for daily peak hours to plot with daily maximum temperature\ndaily_peak_hrs_df <- tou_peak_hours_df |> \n   group_by(date) |> \n   summarize(daily_energy_mwh = sum(hourly_energy_mwh))\n\n# plotting daily peak energy demand with daily max temperatures\npeak_demand_plot <- ggplot(data = daily_peak_hrs_df,\n       aes(x = date, \n           y = daily_energy_mwh)) +\n  geom_line(col = \"#b52b8c\") +\n  labs(title = \"Hourly Energy Demand (MWh)\",\n       x = \"Date\",\n       y = \"MWh\") +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5))\npeak_demand_plot\n\n\n\n\n\ncheckout the code\n# plotting along with daily temperature\nggarrange(peak_demand_plot, max_temp_plot,\n                    ncol = 2, nrow = 1)\n\n\n\n\n\ncheckout the code\n# restructuring the eia data to merge the dataset with the temperature data by date\neia_data <- eia_df |> \n  mutate(time = (date)) |> \n  mutate(date = as.Date(date))\neia_data$time <- format(eia_data$time, format = \"%H:%M:%S\")\n\n# merging the data into one dataframe\nenergy_temp_df <- left_join(x = eia_data,\n                            y = temp_data,\n                            by = \"date\")\n\n\n\n\nanalysis\nA multi-linear regression and time series decomposition analysis can help answer the question at hand. Prior to conducting the linear model, I also used summary statistics to a cutoff point for a dichotomous variable.\n\nlinear model\nTo investigate if the implementation of the TOU policy had an effect on energy demand, I used a multiple linear regression model. However, since other factors also have an effect on electricity demand, I added the temperature and hour of the day on hourly electricity demand. The equation for this model is: \\[hwy_i =\\beta_{0}+\\beta_{1} \\cdot TOUPolicy_i +\\beta_{2} \\cdot \\text HotDay_i+ \\beta_{3} \\cdot \\text PeakHour_i +\\varepsilon_i\\]\nThe ‘TOUPolicy’ predictor (‘tou_policy’ in the results) is a dichotomous variable indicating if the TOU Policy was in effect or not. The ‘PeakHour’ predictor is also a dichotomous variable which indicates whether or not the hour of the day was during peak times from 4:00 - 9:00 p.m.\nLastly, the ‘HotDay’ variable (‘hot_day’ in the results) is a dichotomous variable indicating if the maximum temperature for the San Diego region was equal to or greater than 80 (°F) or below 80 (°F). This cutoff temperature was determined by looking at the mean and standard deviation of the maximum temperature in San Diego during the time of interest. Outlined below in the boxplot, the average maximum temperature was about 72 (°F) and the standard deviation about 7 (°F). thus, 80 (°F) was determined to be a ‘hot day’ in looking at the effect of temperature and hourly electricity demand.\n\n\ncheckout the code\n### ---- Determining a \"Hot Day\" ---- \n\n# determining the mean and standard deviation for the time period of interest\nmean_max_temp <- mean(energy_temp_df$temp_max, na.rm = TRUE)\nsd_max_temp <- sd(energy_temp_df$temp_max, na.rm = TRUE)\n\nprint(mean_max_temp)\nprint(sd_max_temp)\n\n# preparing the data to plot\nbox_data <- as_tibble(energy_temp_df$temp_max)\n\n# plotting the mean and standard deviation\ntemp_box <- ggplot(box_data) +\n  geom_boxplot(aes(x = value)) +\n  labs(x = \"Maximum Daily Temperature (°F)\") +\n  theme_minimal()\n\ntemp_box\n\n\n\n\n\ncheckout the code\n### ---- Adding a 'Hot Day' Indicator in the Dataframe ---- \ntemp_demand_daily <- energy_temp_df |> \n  group_by(date) |> \n  summarize(daily_energy_mwh = sum(hourly_energy_mwh)) |> \n  left_join(temp_data, by = \"date\") |> \n  mutate(hot_day = case_when(\n    (temp_max >= 80) ~ 1,\n    (temp_max <= 79) ~ 0))\n\n### ----- Adding TOU Policy and Peak Hours to Dataframe -----\n\n# adding a year separate year column in the dataframe\nenergy_temp_df <- energy_temp_df |> \n  mutate(year = date)\n\nenergy_temp_df$year <- format(energy_temp_df$year, format = \"%Y\") \n\n# using variables to create dichotomous predictors\nenergy_temp_df <- energy_temp_df |> \n  mutate(tou_policy = case_when(\n    (year > 2020) ~ 1,\n    (year <= 2020) ~ 0)) |> \n  mutate(time = as_datetime(time, format = \"%H:%M:%S\")) |> \n  mutate(time = lubridate::hour(time)) |> \n  mutate(tou_policy = case_when(\n    (year > 2020) ~ 1,\n    (year <= 2020) ~ 0)) |> \n  mutate(peak_hours = case_when(\n    (time < 16) ~ 0,\n    (time >= 16 & time <= 21 ) ~ 1,\n    (time > 21) ~0)) |> \n  mutate(hot_day = case_when(\n    (temp_max >= 80) ~ 1,\n    (temp_max <= 79) ~ 0))\n\n#### ----- Linear Regression on Hourly Energy Demand ---- ###\nmodel_tou_peak_demand <- lm(formula = hourly_energy_mwh ~ \n                              tou_policy + \n                              peak_hours +\n                              hot_day, \n                            data = energy_temp_df)\n\n\n\n\ntime series analysis\nTo dive furthering into additional affects on energy demand, I conducted a classical decomposition analysis to look into other factors influence hourly electricity demand, such as seasonality or overall energy demand trends.\n\n\ncheckout the code\nx = seq(from = ymd('2018-07-1'), \n        length.out = 1481,\n        by='day')\n\n# preparing the dataframe for the time series\ndecom_df <- energy_temp_df |>\n  group_by(date) |>\n  summarize(daily_energy_mwh = sum(hourly_energy_mwh)) |> \n  mutate(index = x)\n\ndecom_ts <- as_tsibble(decom_df, index = index)\n\ndecom_plot_annual <- model(decom_ts, \n                    classical_decomposition(daily_energy_mwh ~ \n                                              season(365), \n                                            type = \"additive\")) |> \n  components() |> \n  autoplot(col = \"#3d405b\") +\n  theme_minimal() +\n  labs(title = \"Classical Decomposition Model\",\n       subtitle = \"Seasonality defined as 365 days\",\n       x = \"Date\",\n       caption = \"Figure 3\")\n\ndecom_plot_monthly <- model(decom_ts, \n                    classical_decomposition(daily_energy_mwh ~ \n                                              season(30), \n                                            type = \"additive\")) |> \n  components() |> \n  autoplot(col = \"#3d405b\") +\n  theme_minimal() +\n  labs(title = \"Classical Decomposition Model\",\n       subtitle = \"Seasonality defined as 30 days\",\n       x = \"Date\",\n       caption = \"Figure 2\")\n\n\n\n\n\nresults\nMultiple Linear Regression:\nWe can interpret all of the parameters used in the regression are significant predictors for hourly electricity demand at a significance level of 0.001 as they all had a p-value of 2 x e-16. The model indicates that when daily maximum temperature is below 80 °F, for non-peak hours and prior to the Time-of-Use implementation in 2020, the average hourly electricity demand is about 2,164 MWh for the SDGE’s service territory. In addition, we expect to see on average a decrease in hourly electricity demand by about 108 MWh for years after the Time-of-Use energy policy was implemented, holding all other predictors constant. For days in which the maximum temperature above 80 °F, the model predicts that the average hourly electricity demand increases by about 409 MWh holding all other predictors constant.\nInterestingly, the model predicts that the average hourly electricity demand decreases by about 360 MWh holding all other predictors constant. At first thought, we may expect too see hourly electricity demand to increase during peak times as these are times in which the time-of-use electricity highlights as times with high demands. However, in this analysis we didn’t look at the amount of renewable electricity available on the grid. Thus, overall energy demand may be lower during peak times but it is possible the percent of average hourly electricity demand in relation to hourly renewable electricity available on the grid may be higher during peak times than non-peak times.\nTable 1 highlights the estimates, p-value and confidence interval for each of the predictors and intercept and the following equation for the linear regression model is:\n\\[Hourly EnergyDemand = 2164 - {108} \\cdot TOUPolicy_i - {360} \\cdot \\text HotDay_i+ {409} \\cdot \\text PeakHour_i +\\varepsilon_i\\]\n\n\ncheckout the code\ntab_model(model_tou_peak_demand,\n          pred.labels = c(\"Intercept\", \n                          \"TOU Policy In Effect\", \n                          \"During Peak Hours\", \n                          \"Max. Temp above 80 (°F)\"),\n          dv.labels = c(\"Hourly Electricity Demand (MWh)\"),\n          string.ci = \"Conf. Int (95%)\",\n          string.p = \"P-value\",\n          title = \"Table 1. Linear Model Results for Predictors on Hourly Electricity Demand\",\n          digits = 2)\n\n\n\n\nTable 1. Linear Model Results for Predictors on Hourly Electricity Demand\n\n \nHourly Electricity Demand (MWh)\n\n\nPredictors\nEstimates\nConf. Int (95%)\nP-value\n\n\nIntercept\n2164.03\n2157.53 – 2170.52\n<0.001\n\n\nTOU Policy In Effect\n-107.82\n-116.89 – -98.75\n<0.001\n\n\nDuring Peak Hours\n-360.18\n-370.32 – -350.03\n<0.001\n\n\nMax. Temp above 80 (°F)\n409.10\n396.47 – 421.74\n<0.001\n\n\nObservations\n36006\n\n\nR2 / R2 adjusted\n0.213 / 0.213\n\n\n\n\n\n\ncheckout the code\nggPredict(model_tou_peak_demand, \n          jitter = TRUE, \n          interactive = TRUE)\n\n\n\n\n\n\nThis figure above graphs the magnitude of the decrease for the TOU policy for each of the outcomes of the other predictors.\nTime Series Analysis - Classical Decomposition:\nTo better understand possible seasonal and overall trends in hourly electricity demand, we can look at a classical decomposition graphs for our time series data for both yearly and monthly seasonality (Figure 2 and 3).\n\n\ncheckout the code\nggarrange(decom_plot_monthly, decom_plot_annual,\n                    ncol = 1, nrow = 2)\n\n\nWarning: Removed 15 rows containing missing values (`geom_line()`).\n\n\nWarning: Removed 182 rows containing missing values (`geom_line()`).\n\n\n\n\n\nLooking at the graphs there doesn’t appear to be evidence of a long-run trend in hourly energy demand over time period analyzed as the trend seems to be mostly constant when seasonality is defined as 30 days and 365 days. It also appears that seasonality may be important in driving overall variation in electricity demand when seasonality is defined as 365 days since the gray bar is closer in scale to the overall time series graph. Anecdotaly , this is intuitive as we can predict that the variance in hourly electricity could be affected by the month. Since month of the year and temperature are it is logical how seasonality based on month affects energy demand given the known relationship on temperature and energy. However, when seasonality is defined as 30 days, the seasonal effect appears to be not as important in the driving overall variation in electricity demand.\n\n\ndiscussion & conclusion\n\n\nsupporting figures & links\nTo see the full repository, check out the project on GitHub at:\nhttps://github.com/colleenmccamy/tou-analysis\nQQ Plot for hourly energy demand residuals.\n\n\ncheckout the code\naug <- energy_temp_df |>  \n  add_predictions(model_tou_peak_demand) |> \n  mutate(residuals_energy = hourly_energy_mwh - pred)\n\nqqPlot(aug$residuals_energy) \n\n\n\n\n\n[1] 16604 16605\n\n\n\n\nreferences\n\n\n\n\n\nFootnotes\n\n\nCalifornia, State of. 2022. “California Releases World’s First Plan to Achieve Net Zero Carbon Pollution.” California Governor. November 16, 2022. https://www.gov.ca.gov/2022/11/16/california-releases-worlds-first-plan-to-achieve-net-zero-carbon-pollution/. }↩︎\nCalifornia, State of. 2022. “California Releases World’s First Plan to Achieve Net Zero Carbon Pollution.” California Governor. November 16, 2022. https://www.gov.ca.gov/2022/11/16/california-releases-worlds-first-plan-to-achieve-net-zero-carbon-pollution/. }↩︎\nhttp://www.caiso.com/documents/matchingtimeofuseperiodswithgridconditions-fastfacts.pdf }↩︎\n“Time of Use.” n.d. SVCE (blog). Accessed December 3, 2022. https://svcleanenergy.org/time-of-use/. }↩︎\n“California Utilities Prep Nation’s Biggest Time-of-Use Rate Rollout.” n.d. Utility Dive. Accessed December 3, 2022. https://www.utilitydive.com/news/california-utilities-prep-nations-biggest-time-of-use-rate-roll-out/543402/. }↩︎\n“API Dashboard - U.S. Energy Information Administration (EIA).” n.d. Accessed December 3, 2022. https://www.eia.gov/opendata/browser/electricity/rto/region-sub-ba-data. }↩︎\n“Our Company | San Diego Gas & Electric.” n.d. Accessed December 3, 2022. https://www.sdge.com/more-information/our-company. }↩︎\nMiller, Norman L., Katharine Hayhoe, Jiming Jin, and Maximilian Auffhammer. 2008. “Climate, Extreme Heat, and Electricity Demand in California.” Journal of Applied Meteorology and Climatology 47 (6): 1834–44. https://doi.org/10.1175/2007JAMC1480.1. }↩︎\nUS Department of Commerce, NOAA. n.d. “Climate.” NOAA’s National Weather Service. Accessed December 3, 2022. https://www.weather.gov/wrh/Climate?wfo=sgx. }↩︎\n\nCitationBibTeX citation:@online{mccamy2022,\n  author = {Colleen McCamy},\n  title = {Time-of-Use {Energy} {Analysis}},\n  date = {2022-12-03},\n  url = {https://colleenmccamy.github.io/2022-12-03-tou-policy-analysis},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nColleen McCamy. 2022. “Time-of-Use Energy Analysis.”\nDecember 3, 2022. https://colleenmccamy.github.io/2022-12-03-tou-policy-analysis."
  }
]