---
title: "Time-of-Use Energy Analysis"
description: "Exploring energy demand data in light of recent policy changes"
author: 
  - name: Colleen McCamy
    affiliation: MEDS
date: 2022-12-03
categories:  [data science, R, spatial-analysis]
citation: 
  url: https://colleenmccamy.github.io/2022-12-03-tou-policy-analysis
draft: true
format:
  html:
    code-fold: true
    code-summary: "checkout the code"
title-block-banner: energy-meters.jpg
title-block-banner-color: "#FAF7F5"
bibliography: references.bib
image: energy-meters.jpg
---

### the question

#### Did the Time-of-Use electricity rate transition have an effect on peak energy demand in the greater San Diego region?

### introduction

California has ambitious clean energy and decarbonization goals. [^1] To achieve these goals, California will need to increase its current electricity grid capacity by about three times. [^1] Time-of-use electricity rates are a value strategy to help reduce the investment needed for expanding grid capacity and can help maximize the use of renewable resources. [^2]

The Time-of-Use (TOU) energy rate referenced in this analysis establishes lower electricity prices for times when there is more renewable energy supply available and helps to encourage electricity use when generation is cleanest and lowest-cost. Additionally, energy rates are higher during the early evening peak to help promote less energy use during times when renewable energy supply decreases and grid operators need to ramp up generation from fossil-fuel based power plants. For the default residential rate, the higher cost hours or "peak hours" are from 4:00 - 9:00 p.m. [^4]

While many utilities have piloted time-varying rates, the recent California TOU transition was the biggest test of time-based rates yet - automatically switching over 20 million electricity consumers to a TOU rate. [^3] While there has been some initial research and analysis on time-based rates in electricity markets, there are limited results on mass-market transitions and how time-based rates affect total electricity consumption. 

Answering the question, 'did the Time-of-Use electricity rate transition have an effect on peak energy demand in the San Diego region?,' can help provide insight to spur further investigations on time-based rates throughout the state of California and beyond.

[^1]: California, State of. 2022. “California Releases World’s First Plan to Achieve Net Zero Carbon Pollution.” California Governor. November 16, 2022. https://www.gov.ca.gov/2022/11/16/california-releases-worlds-first-plan-to-achieve-net-zero-carbon-pollution/.
}

[^2]: http://www.caiso.com/documents/matchingtimeofuseperiodswithgridconditions-fastfacts.pdf
}

[^3]: “California Utilities Prep Nation’s Biggest Time-of-Use Rate Rollout.” n.d. Utility Dive. Accessed December 3, 2022. https://www.utilitydive.com/news/california-utilities-prep-nations-biggest-time-of-use-rate-roll-out/543402/.
}

[^4]: “Time of Use.” n.d. SVCE (blog). Accessed December 3, 2022. https://svcleanenergy.org/time-of-use/.
}


### the data

[Energy Demand Data]{.underline}
Energy demand data used in this analysis are publicly available and provided by the US Energy Information Administration. The data were downloaded via the API dashboard [^5] and were selected to include the time frame of July 1, 2018 to July 31, 2022, hourly demand by subregion in megawatt hours (MWh) in the local time zone (Pacific), and the San Diego Gas and Electric (SDGE) subregion. SDGE serves 3.7 million people through 1.5 million electric meters covering 4,100 square miles in San Diego and southern Orange counties. The energy demand data are an aggregate electricity demand from all customers throughout SDGE's service territory.[^6] 

```{r,warning=FALSE, message=FALSE, results='hide'}
#loading the necessary libraries
library(dplyr)
library(tidyverse)
library(here)
library(readr)
library(gt)
library(tufte)
library(feasts)
library(janitor)
library(lubridate)
library(broom)
library(tsibble)
library(ggpubr)
library(ggiraph)
library(ggiraphExtra)
library(sjPlot)
library(ggcorrplot)

# setting my root directory
rootdir <- ("/Users/colleenmccamy/Documents/MEDS/EDS_222_Stats/final_project")

# reading in the data
eia_data_raw <- read_csv(paste0(rootdir, "/data/eia_data.csv"))

# cleaning the data to be the two variables of interest
eia_df <- eia_data_raw |> 
  select(date, hourly_energy_mwh) |> 
  na.omit()
  
# creating a time series dataframe
eia_ts <- eia_df |> 
  as_tsibble()

```


[Temperature Data]{.underline}
In California, peak electricity demand and temperature is highly correlated. [^8] As this investigation looks into energy demand, temperature data was added to the analysis. The temperature data used in the following analysis are  publicly available through the NOWData Online Weather Data portal from the National Weather Service, a branch of the National Oceanic and Atmospheric Administration. [^7] The temperature data includes an average of daily maximum, minimum, and average temperature from numerous weather stations throughout San Diego County in Fahrenheit. This analysis uses the maximum daily temperature for the same temporal scale as the energy demand data. 

Since the temperature data is an aggregate of multiple stations throughout San Diego County, this can cause bias as the SDGE service territory covers multiple different temperate regions which may not be accurately represented within the average of the stations. Also, the weather stations are more heavily concentrated towards the coast. This could cause bias in the temperature maximum temperatures.

```{r,warning=FALSE, message=FALSE, results='hide'}
# loading in the temperature data
temp_data <- read_csv(paste0(rootdir, "/data/sd_temp_data.csv"))

# wrangling the data
temp_data <- temp_data |> 
  mutate(temp_max = as.numeric(temp_max)) |> 
  mutate(temp_min = as.numeric(temp_min)) |> 
  mutate(temp_avg = as.numeric(temp_avg)) |> 
  mutate(temp_dept = as.numeric(temp_dept)) |> 
  mutate(date = lubridate::mdy(Date)) |> 
  select(!Date)

```

[Exploratory Data Visualizations]{.underline}
The following figures outline the sum of daily energy demand during peak hours and the daily max temperature. 

```{r,warning=FALSE, message=FALSE, results='hide'}

# exploring the data by plotting energy demand throughout time
energy_demand_plot <- ggplot(data = eia_df,
       aes(x = date, 
           y = hourly_energy_mwh)) +
  geom_line(col = "#b52b8c") +
  labs(title = "Hourly Energy Demand (MWh)",
       x = "Date",
       y = "MWh") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5))

# exploring the data by plotting maximum temperature throughout time
max_temp_plot <- ggplot(temp_data, aes(x = date, y = temp_max)) + 
  geom_line(col = "#52796f") +
  labs(title = "Maximum Temperature per day (F)",
       x = "Date",
       y = "Max Temperature (F)") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5))

# creating dataframe for tou peak horus
tou_peak_hours_df <- energy_temp_df |> 
  filter(time >= 16 & time <= 21)

# grouping it for daily peak hours to plot with daily maximum temperature
daily_peak_hrs_df <- tou_peak_hours_df |> 
   group_by(date) |> 
   summarize(daily_energy_mwh = sum(hourly_energy_mwh))

# plotting daily peak energy demand with daily max temperatures
peak_demand_plot <- ggplot(data = daily_peak_hrs_df,
       aes(x = date, 
           y = daily_energy_mwh)) +
  geom_line(col = "#b52b8c") +
  labs(title = "Hourly Energy Demand (MWh)",
       x = "Date",
       y = "MWh") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5))
peak_demand_plot

# plotting along with daily temperature
ggarrange(peak_demand_plot, max_temp_plot,
                    ncol = 2, nrow = 1)

# restructuring the eia data to merge the dataset with the temperature data by date
eia_data <- eia_df |> 
  mutate(time = (date)) |> 
  mutate(date = as.Date(date))
eia_data$time <- format(eia_data$time, format = "%H:%M:%S")

# merging the data into one dataframe
energy_temp_df <- left_join(x = eia_data,
                            y = temp_data,
                            by = "date")

```


[^5]: “API Dashboard - U.S. Energy Information Administration (EIA).” n.d. Accessed December 3, 2022. https://www.eia.gov/opendata/browser/electricity/rto/region-sub-ba-data.
}
[^6]: “Our Company | San Diego Gas & Electric.” n.d. Accessed December 3, 2022. https://www.sdge.com/more-information/our-company.
}

[^7]: US Department of Commerce, NOAA. n.d. “Climate.” NOAA’s National Weather Service. Accessed December 3, 2022. https://www.weather.gov/wrh/Climate?wfo=sgx.
}

[^8]: Miller, Norman L., Katharine Hayhoe, Jiming Jin, and Maximilian Auffhammer. 2008. “Climate, Extreme Heat, and Electricity Demand in California.” Journal of Applied Meteorology and Climatology 47 (6): 1834–44. https://doi.org/10.1175/2007JAMC1480.1.
}


### Analysis

A multi-linear regression and time series decomposition analysis can help answer the question at hand. 

####Linear Model
To investigate if the implementation of the TOU policy had an effect on energy demand, I used a multiple linear regression model. However, since other factors also have an effect on electricity demand, I added the temperature and hour of the day on hourly electricity demand. The equation for this model is:
$$hwy_i =\beta_{0}+\beta_{1} \cdot TOUPolicy_i +\beta_{2} \cdot \text HotDay_i+ \beta_{3} \cdot \text PeakHour_i +\varepsilon_i$$
The 'TOUPolicy' predictor ('tou_policy' in the results) is a dichotomous variable indicating if the TOU Policy was in effect or not. The 'PeakHour' predictor is also a dichotomous variable which indicates whether or not the hour of the day was during peak times from 4:00 - 9:00 p.m.

In addition, the 'HotDay' variable ('hot_day' in the results) is a dichotomous variable indicating if the maximum temperature for the San Diego region was equal to or greater than 80 (°F) or below 80 (°F). This cutoff temperature was determined by looking at the mean and standard deviation of the maximum temperature in San Diego during the time of interest. Outlined below in the boxplot, the average maximum temperature was about 72 (°F) and the standard deviation about 7 (°F). thus, 80 (°F) was determined to be a 'hot day' in looking at the effect of temperature and hourly electricity demand.

```{r,warning=FALSE, message=FALSE, results='hide'}
### ---- Determining a "Hot Day" ---- 

# determining the mean and standard deviation for the time period of interest
mean_max_temp <- mean(energy_temp_df$temp_max, na.rm = TRUE)
sd_max_temp <- sd(energy_temp_df$temp_max, na.rm = TRUE)

print(mean_max_temp)
print(sd_max_temp)

# preparing the data to plot
box_data <- as_tibble(energy_temp_df$temp_max)

# plotting the mean and standard deviation
temp_box <- ggplot(box_data) +
  geom_boxplot(aes(x = value)) +
  labs(x = "Maximum Daily Temperature (°F)") +
  theme_minimal()

temp_box

### ---- Adding a 'Hot Day' Indicator in the Dataframe ---- 
temp_demand_daily <- energy_temp_df |> 
  group_by(date) |> 
  summarize(daily_energy_mwh = sum(hourly_energy_mwh)) |> 
  left_join(temp_data, by = "date") |> 
  mutate(hot_day = case_when(
    (temp_max >= 80) ~ 1,
    (temp_max <= 79) ~ 0))

### ----- Adding TOU Policy and Peak Hours to Dataframe -----

# adding a year separate year column in the dataframe
energy_temp_df <- energy_temp_df |> 
  mutate(year = date)

energy_temp_df$year <- format(energy_temp_df$year, format = "%Y") 

# using variables to create dichotomous predictors
energy_temp_df <- energy_temp_df |> 
  mutate(tou_policy = case_when(
    (year > 2020) ~ 1,
    (year <= 2020) ~ 0)) |> 
  mutate(time = as_datetime(time, format = "%H:%M:%S")) |> 
  mutate(time = lubridate::hour(time)) |> 
  mutate(tou_policy = case_when(
    (year > 2020) ~ 1,
    (year <= 2020) ~ 0)) |> 
  mutate(peak_hours = case_when(
    (time < 16) ~ 0,
    (time >= 16 & time <= 21 ) ~ 1,
    (time > 21) ~0)) |> 
  mutate(hot_day = case_when(
    (temp_max >= 80) ~ 1,
    (temp_max <= 79) ~ 0))

#### ----- Linear Regression on Hourly Energy Demand ---- ###
model_tou_peak_demand <- lm(formula = hourly_energy_mwh ~ 
                              tou_policy + 
                              peak_hours +
                              hot_day, 
                            data = energy_temp_df)



```

#### Time Series Analysis

To dive furthering into the energy 

```{r,warning=FALSE, message=FALSE, results='hide'}

## ---- Highway Data
# reading in highway data using SQL query
query <- "SELECT * FROM gis_osm_roads_free_1 WHERE fclass='motorway'"

# reading in the highways data with st_read()
highways <- st_read("/Users/colleenmccamy/Documents/MEDS/EDS_223_Spatial_Data/data/assignment3/gis_osm_roads_free_1.gpkg", query = query)


## ---- Houses Data
# defining the query for the houses
query_houses <- "SELECT * FROM gis_osm_buildings_a_free_1 WHERE (type IS NULL AND name IS NULL) OR type in ('residential', 'apartments', 'house', 'static_caravan', 'detached')"

# reading in the highways data with st_read()
houses <- st_read("/Users/colleenmccamy/Documents/MEDS/EDS_223_Spatial_Data/data/assignment3/gis_osm_buildings_a_free_1.gpkg", query = query_houses)
```

Lastly, we use data from the [U.S. Census Bureau's American Community Survey](https://www.census.gov/programs-surveys/acs) for census tracts in 2019 from an ArcGIS file geodatabase. The metadata for each layer is available at [ACS metadata](https://www2.census.gov/geo/docs/maps-data/data/tiger/prejoined/ACSMetadata2011.txt) and this data was downloaded in advance to this investigation.

```{r,warning=FALSE, message=FALSE, results='hide'}
#reading in geometry data and selecting for the layer containing the geometry
census_geom <- st_read("/Users/colleenmccamy/Documents/MEDS/EDS_223_Spatial_Data/data/assignment3/ACS_2019_5YR_TRACT_48_TEXAS.gdb", layer = "ACS_2019_5YR_TRACT_48_TEXAS")

#reading in income data
income_median <- st_read("/Users/colleenmccamy/Documents/MEDS/EDS_223_Spatial_Data/data/assignment3/ACS_2019_5YR_TRACT_48_TEXAS.gdb", layer = "X19_INCOME")

```

[Step 2: Creating a blackout mask]{.underline}

The following code outlines how I created a blackout mask for the Houston area that I could use for identifying impacted homes. We operated under the assumption that any location that experienced a drop of more than 200 nW cm^-2^sr^-1^ experienced a blackout.

```{r,warning=FALSE, message=FALSE, results='hide'}

# combing the data into single stars objects for each day
feb16_tile <- st_mosaic(nl_feb16_t1, nl_feb16_t2)
feb07_tile <- st_mosaic(nl_feb07_t1, nl_feb07_t2)

# adding an indicator of the attributes in the data
feb_16_tile_names = setNames(feb16_tile, "light_16")
feb_07_tile_names = setNames(feb07_tile, "light_07")

# matrix alegbra to calculate the difference light difference between the two dates 
blackout_dif <- feb_07_tile_names - feb_16_tile_names

# #filtering for the differences of a drop less that 200 nW cm-2sr-1 as NA
blackout_mask <- cut(blackout_dif, c(200, Inf), labels = "outage")

# vectorizing the blackout mask and fixing any invalid geometries
blackout_mask_v <- st_as_sf(blackout_mask) |> 
  st_make_valid()

# creating a polygon of Houston's coordinates 
hou_border <- st_polygon(list(rbind(c(-96.5,29), c(-96.5,30.5), c(-94.5, 30.5), c(-94.5,29), c(-96.5,29))))

# converting to an sf object and identifying the coordinate reference system
hou_border_sf <- st_sfc(hou_border, crs = 'EPSG:4326')

# cropping the blackout mask with the Houston polygon
hou_outage_mask_v <- blackout_mask_v[hou_border_sf, ,]

# reprojectting the cropped object to a new crs and converting it as an sf object
hou_outage_mask_v_3083 <- st_transform(hou_outage_mask_v, crs = 'EPSG:3083')
outage_mask_clean <- st_as_sf(hou_outage_mask_v_3083)

```

[Step 3: Excluding highway data]{.underline}

To exclude light from highways, we created a buffer of 200 meters and kept areas in our blackout mask that were greater than 200 meters away from a highway.

```{r,warning=FALSE, message=FALSE, results='hide'}

# selecting the highway geometry data
highways_geom <- highways$geom

# transforming the highway geometries to the consistent crs
highways_geom <- st_transform(highways_geom, crs = 'EPSG:3083')

# creating a buffer zone for highways geometry data of 200 meters
highway_buffer <- st_buffer(x = highways_geom, dist = 200)
highway_buffer <- st_transform(highway_buffer, crs = 'EPSG:3083')

# combining the geometries into one and creating a mask that excludes the highway data
highway_buffer <- st_union(highway_buffer, by_feature = FALSE)
mask_hou_highway <- outage_mask_clean[highway_buffer, , op = st_disjoint]

```

[Step 4: Identifying Impacted Homes]{.underline}

To find the number of impacted homes, the code below outlines how I used the new blackout mask with the highway data to identify homes most likely impacted by the power outage.

```{r,warning=FALSE, message=FALSE, results='hide'}

# transforming houses data to be usable
houses <- st_transform(houses, crs = 'EPSG:3083')
houses_st <- st_as_sf(houses)

# filtering the houses data with the blackout mask
outage_houses <- houses_st[mask_hou_highway, drop = FALSE]

# identifying how many homes were affected
print(paste0("There were ", nrow(outage_houses), " homes affected by the power outage on Feburary 16, 2021."))
```

[Step 4: Investigating Socioeconomic factors]{.underline}

Now that we have information on the houses that were affected we can match this with the socioeconomic census tract information and determine which census tracts were impacted by the power outage.

```{r,warning=FALSE, message=FALSE, results='hide'}
# transforming the census data to be consistent with the crs
census_geom <- st_transform(census_geom, crs = 'EPSG:3083')

#selecting the necessary variables and renaming for clarity
income_med_select <- income_median |> 
  dplyr::select("GEOID", "B19013e1") |> 
  rename(GEOID_Data = GEOID, median_income = B19013e1)

# changing the income object to a data_frame
income_med_select_df <- tibble(income_med_select)

# joining census geometries and median income data
census_data <- left_join(census_geom, 
                         income_med_select, 
                         by = "GEOID_Data")

# transforming both objects to the correct crs
census_data <- st_transform(census_data, crs = 'EPSG:4326')
outage_houses <- st_transform(outage_houses, crs = 'EPSG:4326')

# filtering the census data using the outage houses and adding column indicating that these census tracts were part of a blackout
census_outage <- sf::st_filter(census_data, outage_houses) |> 
  mutate(blackout = 'yes')
```

[Step 5: Comparing the incomes of impacted tracts and unimpacted tracts]{.underline}

It is time to visualize our findings. This code breaks down the data wrangling needed for the visualizations and the maps and plots created to compare which census tracts experienced a blackout vs the census tracts that did not experience a blackout.

```{r,warning=FALSE, message=FALSE, results='hide'}

## --- Wrangling our data for our visualizations -----------

# transforming both objects to the crs 4326 to crop it
census_data <- st_transform(census_data, crs = 'EPSG:4326')
hou_border_sf <- st_transform(hou_border_sf, crs = 'EPSG:4326')

# cropping the census data with the Houston border for filtering
census_data_hou <- census_data[hou_border_sf, ,] 

# transforming census data back to the EPSG:3083 crs
census_data_hou <- st_transform(census_data_hou, crs = 'EPSG:3083')

# selecting necessary columns for houston census data
census_data_hou <- census_data_hou |> 
  dplyr::select("NAMELSAD", "Shape", "median_income", "GEOID_Data")

# selecting necessary columns for outage data by census track
census_outage <- census_outage |> 
  dplyr::select("blackout", "GEOID_Data")
census_outage_map <- census_outage |> 
  dplyr::select("blackout")

# converting census outage data to a dataframe in order to join
census_outage_df <- as.data.frame(census_outage)

# joining census outage data and census data for all of Houston
census_map_data <- left_join(census_data_hou, 
                             census_outage_df, 
                             by = "GEOID_Data")

census_map_data <- census_map_data |> 
  dplyr::select('median_income', 'blackout')

# converting census map data to a dataframe to plot
census_plot_data <- data_frame(census_map_data)

# adding an indicator for homes that didn't experience a blackout
census_plot_data <- census_plot_data |> 
  mutate(blackout = replace(blackout, is.na(blackout), "no"))

# creating a data frame for homes that experienced a blackout to plot
census_plot_data_blackout <- census_plot_data |> 
  dplyr::select("median_income", "blackout") |> 
  filter(blackout == "yes")


# creating a data frame for homes that didn't experienced a blackout to plot
census_plot_data_no_blackout <- census_plot_data |> 
  dplyr::select("median_income", "blackout") |> 
  filter(blackout == "no")

```

```{r}

## ------- Mapping our data ---------------

# changing the view mode to be interactive
tmap_mode("view")

# mapping median income by census track and identifying outages by dots
tm_shape(census_map_data) +
  tm_polygons(col = "median_income",
              palette = c("#227c9d", 
                          "#17c3b2", 
                          "#ffcb77", 
                          "#ffe2b3", 
                          "#feb3b1", 
                          "#fe6d73"),
              textNA = "Missing Income Data", 
              colorNA = "#e4ebea",
              title = "Median Income") +
  tm_shape(census_outage_map) +
  tm_dots(shape = 1,
          title = 'blackout') +
  tm_layout(main.title = "Houston Census Data by Income that Experienced A Power Outage",
            legend.outside = TRUE,
            main.title.size = 1
            )
```

The dots indicate which census tracts were impacted by the blackout.

```{r}

### ---- Plotting our data --------

# plotting census data that experienced a blackout
ggplot(census_plot_data_blackout, aes(x = median_income)) +
  geom_histogram(color = "#3d5a80",fill = "#98c1d9") +
  labs(title = "Median Income for Homes that Experienced a Blackout",
       x = "Median Income",
       y = "Count") +
  theme_minimal()

# plotting census data that didn't experienced a blackout
ggplot(census_plot_data_no_blackout, aes(x = median_income)) +
  geom_histogram(fill = "#81b29a",
                 color = "#335c67") +
  labs(title = "Median Income for Homes that Didn't Experience a Blackout",
       x = "Median Income",
       y = "Count") +
  theme_minimal()

# plotting the comparison data via geom jitter plot
ggplot(census_plot_data, aes(x = blackout, y = median_income)) +
  geom_jitter(width = 0.1,
              height = 0,
              color = "#248577",
              alpha = 0.8) +
  labs(title = "Comparing Median Income for Homes that Experienced a Blackout or Not",
       x = "Experienced Blackout",
       y = "Median Income") +
  theme_minimal()

# loading the summary statistics of our data
summary(census_plot_data_blackout)
summary(census_plot_data_no_blackout)
```

### conclusion & limitations

After identifying the average median income for homes in the Houston metropolitan area that experienced a blackout during Texas's 2021 energy crisis, this study showed that average median income for homes that experienced a blackout was \$71,435 and was higher for the average median income for homes that didn't experience a blackout at \$64,494.

However, this study didn't account for the percentage of homes that fell in lower median income tracks versus the percentage of homes that fell in higher median income census tracks and thus weights all census tracks equally upon calculating the average median income. Further investigations could also group census tracks by income level and identify the percentage of impacted vs non-impacted homes for each income grouping to determine if lower median income levels were disproportionately affected compared to higher median income levels.

In addition, the study excluded homes that were 200 meters from highways. This could disproportionately exclude homes with lower median incomes. In addition, this study only looked at median income factors within census tracts and not other socioeconomic factors or medical vulnerability factors.

This goal of this investigation was to become more familiar with spatial data. The results and findings of this investigation are not final and should not be cited without additional investigations. Overall, I hope this blog post was helpful in learning how different packages and functions can be used for working with spatial data.
