---
title: "Music Matching With Machine Learning"
description: "Predicting music taste based on song features"
author: 
  - name: Colleen McCamy
    affiliation: MEDS
date: 2023-03-27
categories:  [data science, R, modeling, machine leanring]
citation: 
  url: https://colleenmccamy.github.io/2023-03-27-music-matching
draft: false
format:
  html:
    code-fold: false
    code-summary: "checkout the code"
    code-overflow: wrap
    code-block-bg: "#cce0dd"
page-layout: article
#title-block-banner: energy-meters.jpg
#bibliography: references.bib
image: music_cds.jpg
---

::: g-col-4
![](music_cds.jpg)
:::

### the question

#### **How can you use song features to predict song preference?**

### introduction

::: g-col-8
Let's get musical! For this project we are using machine learning to match music preferences with song features. Using the Spotify developer API, Lewis White and I have collected our liked song data and classified for this project.
:::

### the data

::: g-col-8
[Spotify Song Data]{.underline}

The data used in this project was from mine and Lewis's Spotify account. We used song data for all of our liked songs within spotify. This data was downloaded through the Spotify Developer API. In order to use the Spotify you must have a Spotify account. If you don't have one, sign up for a free one here: https://www.spotify.com/us/signup

Once you have an account, go to Spotify for developers (https://developer.spotify.com/) and log in. Click the green "Create a Client ID" button to fill out the form to create an app create an app so you can access the API.

On your developer dashboard page, click on the new app you just created. On the app's dashboard page you will find your Client ID just under the header name of your app. Click "Show Client Secret" to access your secondary Client ID. When you do this you'll be issued a Spotify client ID and client secret key.
:::

::: column-margin
![](guitar_image.jpg)
:::

```{r,warning=FALSE, message=FALSE, results='hide'}
library(spotifyr) #API interaction
library(tidyverse)
library(tidymodels)
library(dplyr)    
library(ggplot2)   
library(rsample)   
library(caret)  
library(recipes)
library(skimr)
library(tidymodels)
library(kknn)
library(baguette)
library(themis)
library(ranger)
library(vip)

```

::: column-margin
![](spotify_screen.jpg)
:::

### Getting the Data through the Spotify API

```{r data exploration, warning=FALSE, message=FALSE, results='hide', eval = FALSE }
# client ID for the Spotify API
Sys.setenv(SPOTIFY_CLIENT_ID = '') # hide your api key within a .env file so you don't publish it
Sys.setenv(SPOTIFY_CLIENT_SECRET = '')

# getting access token
access_token <- get_spotify_access_token() #takes ID and SECRET, sends to Spotify and receives an access token

# getting spotify data
cm_spotify_2 <- ceiling(get_my_saved_tracks(include_meta_info = TRUE)[['total']] / 50) |> 
  seq() |> 
  map(function(x){
    get_my_saved_tracks(limit = 50, offset = (x - 1) * 50)}) |> 
  reduce(rbind) |> 
  write_csv('raw_myFavTracks.cvs')

# selecting first 6,000 rows of spotify data
cm_spotify_2 <- cm_spotify_2[(1:6000),]

# splitting the dataframe into 60 dataframes of 100 rows each
df_list <- split(cm_spotify_2, rep(1:60, each = 100))

# apply get_track_audio_features() to each split dataframe in the list since it can only do 100 requests at a time
feature_list <- lapply(df_list, 
                       function(cm_spotify_2)
                         get_track_audio_features(cm_spotify_2$track.id))

# combine all rows into one dataframe
feature_df <- do.call(rbind, feature_list)

# adding the track name column to the audio features
cm_audio_feat<- cbind(feature_df, cm_spotify_2$track.name) #|> 
   # write_csv('cm_spotify_data.csv') # added write csv to provide Lewis the data

```

[Reading in Lewis's Spotify data]{.underline}\
The following figures outline the sum of peak daily energy demand hours and the daily max temperature from our datasets.

```{r,warning=FALSE, message=FALSE, results='hide'}

cm_audio_feat <- read_csv("/Users/colleenmccamy/Documents/MEDS/classes/winter/eds_232_machine_learning/labs/cm_spotify_data.csv")    

# adding column for Lewis indicator
lw_audio_feat <- read_csv("/Users/colleenmccamy/Documents/MEDS/classes/winter/eds_232_machine_learning/labs/lewis_liked_tracks_full.csv") |> 
  select(-c(listener, primary_artist)) |> 
  add_column(listener = 1) 

```

```{r,warning=FALSE, message=FALSE, results='hide'}
# adding column for Colleen indicator
cm_audio_feat <- cm_audio_feat|> 
  add_column(listener = 0) |> 
  rename(track.name = "cm_spotify_2$track.name")

# joining Lewis's and my audio feature data
audio_feat <- rbind(lw_audio_feat, cm_audio_feat)

```

## data exploration

Looking at what our like music looks like and how similar our taste in music is. Lewis and I have over 76 hours of similar songs! This means we can do a road trip from Santa Barbara to Chicago and back to Santa Barbara only listening to our liked songs with no repeats. And the longest song is 'Note to Self' at over 14 minutes long.

```{r,warning=FALSE, message=FALSE, results='hide'}

# determining the longest track
longest_song <- max(audio_feat$duration_ms)
longest_song_title <- audio_feat$track.name[audio_feat$duration_ms == longest_song]
print(paste0("The longest song is ", "'", longest_song_title, "' at ", round(longest_song/1000/60, 2), " minutes."))

```

```{r,warning=FALSE, message=FALSE, results='hide'}

# creating treemap dataframe
treemap_df <- audio_feat |> 
  mutate(group = case_when(danceability < 0.3 ~ 1,
                           danceability > 0.3 & danceability < 0.6 ~ 2,
                           danceability > 0.6 & danceability < 0.9 ~ 3,
                           danceability > 0.9 & danceability < 1 ~ 4 )) |> 
  mutate(subgroup = case_when(speechiness < 0.3 ~ 1,
                           speechiness > 0.3 & speechiness < 0.6 ~ 2,
                           speechiness > 0.6 & speechiness < 0.9 ~ 3,
                           speechiness > 0.9 & speechiness < 1 ~ 4 )) |>
  group_by(group) |> 
  mutate(dance_count = n()) |> 
  group_by(subgroup) |> 
  mutate(speech_count = n())

different_songs <- distinct(audio_feat, track.name, .keep_all = TRUE)
nrow(different_songs)
similar_songs <- audio_feat[duplicated(audio_feat$track.name) | duplicated(audio_feat$track.name,
                                                                           fromLast = TRUE), ]
similar_songs <- audio_feat |> 
  mutate(duplicate = case_when(duplicated(track.name) == TRUE ~ 1,
                               duplicated(track.name) == FALSE ~ 0)) |> 
  filter(duplicate == 1)

nrow(similar_songs)

roadtrip_min <- sum(similar_songs$duration_ms)/60000
roadtrip_hr <- roadtrip_min/60
roadtrip_hr

# testing to ensure that the similar songs got accounted for
nrow(similar_songs) + nrow(different_songs) == nrow(audio_feat)

```

```{r,warning=FALSE, message=FALSE}

# plotting the treemap
library(treemap) # loading the library
treemap(treemap_df,
            index=c("group","subgroup"),
            vSize="dance_count",
            type="index"
            ) 

```

### conducting different machine learning models

A multi-linear regression model and time series decomposition analysis can help answer the question at hand. Prior to conducting the linear model, I also used summary statistics to a establish cutoff point in creating temperature as a dichotomous variable.

#### [K-Nearest Neighbors]{.underline}

```{r,warning=FALSE, message=FALSE, results='hide'}

############################################
######### Creating the Data Split ##########
# ---------------------------------------- #

# selecting columns for the data set to model
model_dat <- audio_feat |> 
  select(-c("type", "id", "uri", "track_href", 
            "analysis_url", "duration_ms", "time_signature", 
            "track.name", "type")) |> 
  mutate(listener = as.factor(listener)) |> 
  mutate_if(is.ordered, factor, ordered = FALSE)

set.seed(123) # setting seed for reproducibility
#initial split of data, 70/30 split
audio_split <- initial_split(model_dat, 
                             prop = 0.70,
                             strata = "listener")
audio_test <- testing(audio_split)
audio_train <- training(audio_split)

############################################
############ Initiating the Model ##########
# ---------------------------------------- #

# setting the recipe
knn_rec <- recipe(listener ~., data = audio_train) %>%
  step_dummy(all_nominal(),-all_outcomes(),one_hot = TRUE) %>% 
  step_normalize(all_numeric(), -all_outcomes(),)%>%
  prep()

#bake 
baked_audio <- bake(knn_rec, audio_train)

# applying recipe to test data
baked_test <- bake(knn_rec, audio_test)

# specifying nearest neighbor model (not tuned)
knn_spec <- nearest_neighbor(neighbors = 7) |>  
  set_engine("kknn") |>  
  set_mode("classification")

# fitting the specification to the data
knn_fit <- knn_spec |> 
  fit(listener ~ ., data = audio_train)

# setting seed for reproducibility
set.seed(123)

# adding 5-fold cross validation to the training dataset
cv_folds <- audio_train |> vfold_cv(v = 5)

# adding this all to a workflow
knn_workflow <- workflow() |> 
  add_model(knn_spec) |> 
  add_recipe(knn_rec)

# adding resamples to the workflow
knn_res <- knn_workflow |> 
  fit_resamples(
    resamples = cv_folds,
    control = control_resamples(save_pred = TRUE)
  )
  
# checking performance
knn_res |> collect_metrics()

############################################
###### Adding Tuning to the KNN Model ######
# ---------------------------------------- #

# defining the KNN model with tuning for number of neighbors
knn_spec_tune <- 
  nearest_neighbor(neighbors = tune()) |> 
  set_mode("classification") |> 
  set_engine("kknn")

# defining the new workflow
wf_knn_tune <- workflow() |> 
  add_model(knn_spec_tune) |> 
  add_recipe(knn_rec)

# fitting the workflow on the predifined folds and grid of hyperparameters
fit_knn_cv <- wf_knn_tune |> 
  tune_grid(
    cv_folds,
    grid = data.frame(neighbors = c(1,5, seq(10, 100, 10)))
  )

# collecting the model metrics
fit_knn_cv |> collect_metrics()

############################################
########## Finalizing the Model ############
# ---------------------------------------- #

# setting the final workflow with the initial workflow and the best model
final_wf <- wf_knn_tune |> 
  finalize_workflow(select_best(fit_knn_cv, metric = "accuracy"))

# fitting the final workflow with the training data
final_fit <- final_wf |> 
  fit(data = audio_train)

#############################################
# Predicting on the final model (test data) #
# ----------------------------------------- #

# adding last fit to go over the final fit and workflow
audio_knn_final <- final_fit |> 
  last_fit(audio_split)

```

#### MODEL RESULTS

```{r}
# collect the metrics
audio_knn_final |> collect_metrics()
```

#### [decision tree]{.underline}

```{r,warning=FALSE, message=FALSE, results='hide'}

############################################
########## Setting Up the Model ############
# ---------------------------------------- #

# preprocessing the data
tree_rec <- recipe(listener ~., data = audio_train) |> 
  step_dummy(all_nominal(),-all_outcomes(),one_hot = TRUE) |>  
  step_normalize(all_numeric(), -all_outcomes(),) |> 
  prep()
  
tree_rec_down <- recipe(listener ~., data = audio_train) |> 
  step_dummy(all_nominal(),-all_outcomes(),one_hot = TRUE) |>  
  step_normalize(all_numeric(), -all_outcomes(),) |> 
  step_downsample(listener) |> 
  prep()

# tree specification
tree_spec_tune <- decision_tree(
  cost_complexity = tune(), 
  tree_depth = tune(),
  min_n = tune()) |> # tuning minimum node size
  set_engine("rpart") |> 
               set_mode("classification")

# retrieving a tuning grid
tree_grid <- grid_regular(cost_complexity(),
                          tree_depth(),
                          min_n(),
                          levels = 5)

# setting up the decision tree workflow
wf_tree_tune <- workflow() |> 
  add_recipe(tree_rec_down) |> 
  add_model(tree_spec_tune)

############################################
########## Run & Tune the Model ############
# ---------------------------------------- #

# setting up computing to run in parrellel 
doParallel::registerDoParallel()

# using tune grid to run the model
tree_rs <-tune_grid(
  tree_spec_tune,
  listener ~ ., 
  resamples = cv_folds,
  grid = tree_grid,
  metrics = metric_set(accuracy)
  )

############################################
###### Finalizing & Fitting the Model ######
# ---------------------------------------- #

# selecting the best tree parameters
best_param <- select_best(tree_rs)

# finalizing the model
final_tree <- finalize_model(tree_spec_tune, best_param)

# fitting the final model with the data 
final_tree_fit <- last_fit(final_tree, listener ~ ., audio_split)

# seeing the predictions
final_tree_fit$.predictions
```

#### MODEL RESULTS

```{r}

# collecting metrics of the decision tree model
final_tree_fit |> collect_metrics()

```

#### [bagged tree]{.underline}

```{r,warning=FALSE, message=FALSE, results='hide'}
############################################
########## Setting Up the Model ############
# ---------------------------------------- #

# specification for the bagged tree model
tree_bag_spec <- bag_tree(cost_complexity = tune(),
                          tree_depth = tune(),
                          min_n = tune()) |> 
  set_engine("rpart", times = 50) |> 
  set_mode("classification")

# creating the bagged tree workflow
tree_bag_wf <- workflow() |> 
  add_recipe(tree_rec_down) |> 
  add_model(tree_bag_spec)

# setting the grid
tree_bag_grid <- grid_regular(cost_complexity(),
                              tree_depth(),
                              min_n(),
                              levels = 5)

############################################
########## Run & Tune the Model ############
# ---------------------------------------- #

# parallel computing for speed
doParallel::registerDoParallel()

# fitting the data to the bagged tree specification
tree_bag_rs <- tune_grid(
  tree_bag_wf,
  listener ~ .,
  resamples = cv_folds,
  grid = tree_bag_grid,
  metrics = metric_set(accuracy)
  )

############################################
###### Finalizing & Fitting the Model ######
# ---------------------------------------- #

# selecting the best tree parameters
best_bag_param <- select_best(tree_bag_rs)

# finalizing the model
final_tree <- finalize_model(tree_bag_spec, best_bag_param)

# fitting the final model with the data 
final_tree_bag_fit <- last_fit(final_tree, listener ~ ., audio_split)

# seeing the predictions
final_tree_bag_fit$.predictions

```

#### MODEL RESULTS

```{r}
# collecting metrics of the decision tree model
final_tree_bag_fit |> collect_metrics()

```

#### [random forest]{.underline}

Diving deeper into additional effects on energy demand, I conducted a classical decomposition analysis to investigate if seasonality or any overall trends influence hourly electricity demand within the time frame we are interested in.

```{r,warning=FALSE, message=FALSE, results='hide'}

# defining the model
rand_spec <- rand_forest(
  mtry = tune(),
  trees = 1000,
  min_n = tune()) |> 
  set_mode("classification") |> 
  set_engine("ranger")


# setting up a workflow
rand_wf <- workflow() |> 
  add_recipe(tree_rec_down) |> 
  add_model(rand_spec)


############################################
########## Run & Tune the Model ############
# ---------------------------------------- #

doParallel::registerDoParallel()

set.seed(123)

# creating grid for turning on folds
rand_res <- tune_grid(
  rand_wf,
  resamples = cv_folds,
  grid = 20
)
```

```{r}

# plotting the tuning results
rand_res |> collect_metrics() |> 
  filter(.metric == "accuracy") |> 
  ggplot(aes(x = mtry, 
             y = mean, 
             color = min_n)) +
  geom_point(show.legend = TRUE) + 
  theme_minimal()

```

```{r,warning=FALSE, message=FALSE, results='hide'}

############################################
###### Finalizing & Fitting the Model ######
# ---------------------------------------- #

# selecting the best model
best_rand <- select_best(rand_res)


# finalizing the model
final_rand <- finalize_model(
  rand_spec,
  best_rand
)

```

```{r}

final_rand |> 
  set_engine("ranger",
             importance = "permutation") |>
  fit(listener ~ .,
      data = audio_train) |> 
  vip(geom = "point") + theme_minimal()

```

#### MODEL RESULTS

```{r}

# verifying model on testing data
final_rand_wf <- workflow() |> 
  add_recipe(tree_rec_down) |> 
  add_model(final_rand)

rand_result <- final_rand_wf |> 
  last_fit(audio_split)

rand_result |> collect_metrics()

```

### discussion & conclusion

With this initial analysis we can conclude that the TOU energy rate policy had an effect on hourly electricity demand in SDGE's service territory, and had a greater effect during peak hours. However, we can also see that other factors not addressed in the model account for more of the variation in hourly electricity demand.

Additional research and analysis should be done to determine how the TOU policy affects electricity demand in relation to the other factors not addressed in this analysis. This can include dividing electricity demand by customer type or rate class, as this analysis just uses an aggregated energy demand for all customers. Furthermore, this analysis was conducted with the assumption that all SDGE customers transitioned to a TOU rate that had peak hours from 4:00 - 9:00 p.m. (TOU-C rate) and were transitioned all at the same time. We know that this is not the case. SDGE conducted a rolling transition throughout the year 2020 and not all customers were transitioned to the TOU-C rate. Conducting this analysis with a more accurate indication of TOU implementation per customer is needed. However, this type information is not publicly available by utilities as it can include confidential customer information.

This analysis is also spatially limited for SDGE's service area. Additional research can expand this investigation statewide. Lastly, this investigation just looks at electricity demand. However, as noted previously a key part of TOU policy is to reduce energy use when demand is high and renewable supply is low. Instead of solely using hourly electricity demand as the outcome variable, future research can look into how the TOU policy affects energy demand when renewable supply is low.

Given additional analysis is conducted, this information can be used to inform policy makers, energy providers (load-serving entities) and grid operators about the effectiveness of the TOU policy and how the policy can continue to support California's clean energy goals.

### supporting figures & links

To see the full repository, check out the project on Github at:\
[https://github.com/colleenmccamy/tou-energy-analysis](https://github.com/colleenmccamy/tou-energy-analysishttps://github.com/colleenmccamy/tou-energy-analysis)

Table 1:

```{r}
tab_model(model_tou_peak_demand,
          pred.labels = c("Intercept", 
                          "TOU Policy In Effect", 
                          "During Peak Hours", 
                          "Max. Temp above 80 (°F)"),
          dv.labels = c("Hourly Electricity Demand (MWh)"),
          string.ci = "Conf. Int (95%)",
          string.p = "P-value",
          title = "Table 1. Linear Model Results for Predictors on Hourly Electricity Demand",
          digits = 2)
```

Table 2:

```{r}

tab_model(model_int_tou_peak_demand,
          pred.labels = c("Intercept", 
                          "TOU Policy In Effect", 
                          "During Peak Hours", 
                          "Max. Temp above 80 (°F)",
                          "TOU Policy & Peak Hours"),
          dv.labels = c("Hourly Electricity Demand (MWh)"),
          string.ci = "Conf. Int (95%)",
          string.p = "P-value",
          title = "Table 2. Linear Model Results for Predictors on Hourly Electricity Demand with an Interaction Addition",
          digits = 2)

```

QQ Plot for hourly energy demand residuals: This supports that a linear model is an appropriate method in conducting our analysis as the residual from the model predictions appear to be mainly normal.

Figure 5:

```{r,warning=FALSE, message=FALSE}

aug <- energy_temp_df |>  
  add_predictions(model_int_tou_peak_demand) |> 
  mutate(residuals_energy = hourly_energy_mwh - pred)

qqPlot(aug$residuals_energy) 

```

Box plot for exploring Maximum Temperature Data:

Figure 6:

```{r,warning=FALSE, message=FALSE}
# plotting the mean and standard deviation
temp_box <- ggplot(box_data) +
  geom_boxplot(aes(x = value), col = "#300e2e",
               fill = "#8a6d88") +
  labs(x = "Maximum Daily Temperature (°F)") +
  theme_minimal()

temp_box
```

### references
